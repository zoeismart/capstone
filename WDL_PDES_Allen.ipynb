{"cells":[{"cell_type":"markdown","metadata":{"id":"ZkEQbs8AJwWH"},"source":["##### WLD SOLVER"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HyEEHbtd7dJx","executionInfo":{"status":"ok","timestamp":1728522862587,"user_tz":420,"elapsed":1387,"user":{"displayName":"xi zhao","userId":"00460399834464454081"}},"outputId":"18484bd9-d1c2-4673-acee-13c1ffef91f7"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uJeCw7kaJwWJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728522867312,"user_tz":420,"elapsed":4727,"user":{"displayName":"xi zhao","userId":"00460399834464454081"}},"outputId":"f803775e-27ec-4bfd-f003-5724fed39a83"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n","2.17.0\n"]}],"source":["import tensorflow as tf\n","device_name = tf.test.gpu_device_name()\n","if device_name != '/device:GPU:0':\n","     raise SystemError('GPU device not found')\n","print('Found GPU at: {}'.format(device_name))\n","print(tf.__version__)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"iIDPN9CBJwWP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1728528491576,"user_tz":420,"elapsed":5624267,"user":{"displayName":"xi zhao","userId":"00460399834464454081"}},"outputId":"8998fa74-f205-4b89-a17f-5ac2d4a7c197"},"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","non-resource variables are not supported in the long term\n","WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n","Instructions for updating:\n","keep_dims is deprecated, use keepdims instead\n"]},{"output_type":"stream","name":"stdout","text":["0 run:\n"," Begin to solve Allen - Cahn equation \n","step :     0 , loss : 1.6158e-01 , Y0 :  5.4381e-01 , runtime :   18 \n","step :   100 , loss : 1.2692e-01 ,  Y0 :  4.9516e-01 , runtime :   52 \n","step :   200 , loss : 9.9422e-02 ,  Y0 :  4.5004e-01 , runtime :   82 \n","step :   300 , loss : 7.8032e-02 ,  Y0 :  4.0832e-01 , runtime :  109 \n","step :   400 , loss : 6.0837e-02 ,  Y0 :  3.6975e-01 , runtime :  139 \n","step :   500 , loss : 4.7267e-02 ,  Y0 :  3.3401e-01 , runtime :  168 \n","step :   600 , loss : 3.6248e-02 ,  Y0 :  3.0103e-01 , runtime :  196 \n","step :   700 , loss : 2.7612e-02 ,  Y0 :  2.7048e-01 , runtime :  226 \n","step :   800 , loss : 2.0776e-02 ,  Y0 :  2.4241e-01 , runtime :  255 \n","step :   900 , loss : 1.5424e-02 ,  Y0 :  2.1670e-01 , runtime :  284 \n","step :  1000 , loss : 1.1286e-02 ,  Y0 :  1.9333e-01 , runtime :  312 \n","step :  1100 , loss : 8.1860e-03 ,  Y0 :  1.7231e-01 , runtime :  341 \n","step :  1200 , loss : 5.8499e-03 ,  Y0 :  1.5347e-01 , runtime :  369 \n","step :  1300 , loss : 4.1310e-03 ,  Y0 :  1.3674e-01 , runtime :  398 \n","step :  1400 , loss : 2.8756e-03 ,  Y0 :  1.2207e-01 , runtime :  428 \n","step :  1500 , loss : 1.9983e-03 ,  Y0 :  1.0931e-01 , runtime :  455 \n","step :  1600 , loss : 1.3633e-03 ,  Y0 :  9.8167e-02 , runtime :  483 \n","step :  1700 , loss : 9.4596e-04 ,  Y0 :  8.8812e-02 , runtime :  511 \n","step :  1800 , loss : 6.7102e-04 ,  Y0 :  8.1035e-02 , runtime :  539 \n","step :  1900 , loss : 4.9297e-04 ,  Y0 :  7.4495e-02 , runtime :  569 \n","step :  2000 , loss : 3.8360e-04 ,  Y0 :  6.9254e-02 , runtime :  597 \n","step :  2100 , loss : 3.2231e-04 ,  Y0 :  6.5189e-02 , runtime :  626 \n","step :  2200 , loss : 2.8468e-04 ,  Y0 :  6.1910e-02 , runtime :  658 \n","step :  2300 , loss : 2.6481e-04 ,  Y0 :  5.9353e-02 , runtime :  687 \n","step :  2400 , loss : 2.5295e-04 ,  Y0 :  5.7423e-02 , runtime :  716 \n","step :  2500 , loss : 2.4714e-04 ,  Y0 :  5.6087e-02 , runtime :  744 \n","step :  2600 , loss : 2.4419e-04 ,  Y0 :  5.5164e-02 , runtime :  773 \n","step :  2700 , loss : 2.3596e-04 ,  Y0 :  5.4374e-02 , runtime :  800 \n","step :  2800 , loss : 2.3401e-04 ,  Y0 :  5.3835e-02 , runtime :  827 \n","step :  2900 , loss : 2.3579e-04 ,  Y0 :  5.3552e-02 , runtime :  854 \n","step :  3000 , loss : 2.3458e-04 ,  Y0 :  5.3180e-02 , runtime :  879 \n","step :  3100 , loss : 2.3080e-04 ,  Y0 :  5.3086e-02 , runtime :  906 \n","step :  3200 , loss : 2.2870e-04 ,  Y0 :  5.3076e-02 , runtime :  933 \n","step :  3300 , loss : 2.2533e-04 ,  Y0 :  5.2935e-02 , runtime :  962 \n","step :  3400 , loss : 2.2213e-04 ,  Y0 :  5.3084e-02 , runtime :  988 \n","step :  3500 , loss : 2.1995e-04 ,  Y0 :  5.3003e-02 , runtime : 1014 \n","step :  3600 , loss : 2.1875e-04 ,  Y0 :  5.3008e-02 , runtime : 1040 \n","step :  3700 , loss : 2.1324e-04 ,  Y0 :  5.2999e-02 , runtime : 1069 \n","step :  3800 , loss : 2.1230e-04 ,  Y0 :  5.2901e-02 , runtime : 1097 \n","step :  3900 , loss : 2.0667e-04 ,  Y0 :  5.2922e-02 , runtime : 1124 \n","step :  4000 , loss : 1.9803e-04 ,  Y0 :  5.3185e-02 , runtime : 1151 \n"," running time :  1151.959 s \n","1 run:\n"," Begin to solve Allen - Cahn equation \n","step :     0 , loss : 1.5930e-01 , Y0 :  5.4381e-01 , runtime :   16 \n","step :   100 , loss : 1.2452e-01 ,  Y0 :  4.9533e-01 , runtime :   49 \n","step :   200 , loss : 9.7678e-02 ,  Y0 :  4.5050e-01 , runtime :   77 \n","step :   300 , loss : 7.6797e-02 ,  Y0 :  4.0891e-01 , runtime :  104 \n","step :   400 , loss : 6.0123e-02 ,  Y0 :  3.7022e-01 , runtime :  133 \n","step :   500 , loss : 4.6721e-02 ,  Y0 :  3.3442e-01 , runtime :  159 \n","step :   600 , loss : 3.5902e-02 ,  Y0 :  3.0137e-01 , runtime :  185 \n","step :   700 , loss : 2.7509e-02 ,  Y0 :  2.7090e-01 , runtime :  211 \n","step :   800 , loss : 2.0774e-02 ,  Y0 :  2.4277e-01 , runtime :  237 \n","step :   900 , loss : 1.5534e-02 ,  Y0 :  2.1718e-01 , runtime :  264 \n","step :  1000 , loss : 1.1439e-02 ,  Y0 :  1.9381e-01 , runtime :  289 \n","step :  1100 , loss : 8.2958e-03 ,  Y0 :  1.7261e-01 , runtime :  317 \n","step :  1200 , loss : 5.9250e-03 ,  Y0 :  1.5379e-01 , runtime :  346 \n","step :  1300 , loss : 4.1961e-03 ,  Y0 :  1.3695e-01 , runtime :  374 \n","step :  1400 , loss : 2.9337e-03 ,  Y0 :  1.2219e-01 , runtime :  402 \n","step :  1500 , loss : 2.0288e-03 ,  Y0 :  1.0936e-01 , runtime :  429 \n","step :  1600 , loss : 1.4079e-03 ,  Y0 :  9.8317e-02 , runtime :  455 \n","step :  1700 , loss : 9.8597e-04 ,  Y0 :  8.8880e-02 , runtime :  481 \n","step :  1800 , loss : 7.1103e-04 ,  Y0 :  8.1026e-02 , runtime :  506 \n","step :  1900 , loss : 5.3091e-04 ,  Y0 :  7.4552e-02 , runtime :  533 \n","step :  2000 , loss : 4.1767e-04 ,  Y0 :  6.9284e-02 , runtime :  562 \n","step :  2100 , loss : 3.5305e-04 ,  Y0 :  6.5126e-02 , runtime :  589 \n","step :  2200 , loss : 3.1100e-04 ,  Y0 :  6.1826e-02 , runtime :  616 \n","step :  2300 , loss : 2.9031e-04 ,  Y0 :  5.9363e-02 , runtime :  643 \n","step :  2400 , loss : 2.7950e-04 ,  Y0 :  5.7297e-02 , runtime :  670 \n","step :  2500 , loss : 2.7451e-04 ,  Y0 :  5.5945e-02 , runtime :  698 \n","step :  2600 , loss : 2.6895e-04 ,  Y0 :  5.5078e-02 , runtime :  726 \n","step :  2700 , loss : 2.6529e-04 ,  Y0 :  5.4382e-02 , runtime :  755 \n","step :  2800 , loss : 2.6371e-04 ,  Y0 :  5.3792e-02 , runtime :  784 \n","step :  2900 , loss : 2.6166e-04 ,  Y0 :  5.3385e-02 , runtime :  812 \n","step :  3000 , loss : 2.6181e-04 ,  Y0 :  5.3316e-02 , runtime :  840 \n","step :  3100 , loss : 2.5614e-04 ,  Y0 :  5.3160e-02 , runtime :  866 \n","step :  3200 , loss : 2.5432e-04 ,  Y0 :  5.3170e-02 , runtime :  892 \n","step :  3300 , loss : 2.5264e-04 ,  Y0 :  5.2862e-02 , runtime :  918 \n","step :  3400 , loss : 2.4919e-04 ,  Y0 :  5.2962e-02 , runtime :  944 \n","step :  3500 , loss : 2.4435e-04 ,  Y0 :  5.2923e-02 , runtime :  972 \n","step :  3600 , loss : 2.3898e-04 ,  Y0 :  5.2894e-02 , runtime : 1001 \n","step :  3700 , loss : 2.3741e-04 ,  Y0 :  5.2717e-02 , runtime : 1028 \n","step :  3800 , loss : 2.3440e-04 ,  Y0 :  5.2641e-02 , runtime : 1055 \n","step :  3900 , loss : 2.3033e-04 ,  Y0 :  5.2786e-02 , runtime : 1083 \n","step :  4000 , loss : 2.2508e-04 ,  Y0 :  5.3058e-02 , runtime : 1107 \n"," running time :  1107.523 s \n","2 run:\n"," Begin to solve Allen - Cahn equation \n","step :     0 , loss : 1.6276e-01 , Y0 :  5.4381e-01 , runtime :   16 \n","step :   100 , loss : 1.2756e-01 ,  Y0 :  4.9525e-01 , runtime :   49 \n","step :   200 , loss : 1.0019e-01 ,  Y0 :  4.5039e-01 , runtime :   77 \n","step :   300 , loss : 7.8725e-02 ,  Y0 :  4.0884e-01 , runtime :  105 \n","step :   400 , loss : 6.1393e-02 ,  Y0 :  3.7021e-01 , runtime :  130 \n","step :   500 , loss : 4.7543e-02 ,  Y0 :  3.3439e-01 , runtime :  158 \n","step :   600 , loss : 3.6465e-02 ,  Y0 :  3.0116e-01 , runtime :  184 \n","step :   700 , loss : 2.7748e-02 ,  Y0 :  2.7067e-01 , runtime :  211 \n","step :   800 , loss : 2.0873e-02 ,  Y0 :  2.4263e-01 , runtime :  237 \n","step :   900 , loss : 1.5542e-02 ,  Y0 :  2.1686e-01 , runtime :  261 \n","step :  1000 , loss : 1.1411e-02 ,  Y0 :  1.9348e-01 , runtime :  288 \n","step :  1100 , loss : 8.2318e-03 ,  Y0 :  1.7232e-01 , runtime :  317 \n","step :  1200 , loss : 5.8721e-03 ,  Y0 :  1.5338e-01 , runtime :  343 \n","step :  1300 , loss : 4.1262e-03 ,  Y0 :  1.3658e-01 , runtime :  370 \n","step :  1400 , loss : 2.8737e-03 ,  Y0 :  1.2190e-01 , runtime :  397 \n","step :  1500 , loss : 1.9690e-03 ,  Y0 :  1.0910e-01 , runtime :  424 \n","step :  1600 , loss : 1.3649e-03 ,  Y0 :  9.8080e-02 , runtime :  451 \n","step :  1700 , loss : 9.5945e-04 ,  Y0 :  8.8824e-02 , runtime :  478 \n","step :  1800 , loss : 6.9258e-04 ,  Y0 :  8.0929e-02 , runtime :  506 \n","step :  1900 , loss : 5.2477e-04 ,  Y0 :  7.4504e-02 , runtime :  535 \n","step :  2000 , loss : 4.2617e-04 ,  Y0 :  6.9261e-02 , runtime :  562 \n","step :  2100 , loss : 3.6884e-04 ,  Y0 :  6.5169e-02 , runtime :  590 \n","step :  2200 , loss : 3.3454e-04 ,  Y0 :  6.1896e-02 , runtime :  616 \n","step :  2300 , loss : 3.1515e-04 ,  Y0 :  5.9337e-02 , runtime :  641 \n","step :  2400 , loss : 3.0790e-04 ,  Y0 :  5.7516e-02 , runtime :  667 \n","step :  2500 , loss : 3.0533e-04 ,  Y0 :  5.6133e-02 , runtime :  695 \n","step :  2600 , loss : 3.0358e-04 ,  Y0 :  5.5056e-02 , runtime :  722 \n","step :  2700 , loss : 2.9976e-04 ,  Y0 :  5.4486e-02 , runtime :  751 \n","step :  2800 , loss : 2.9842e-04 ,  Y0 :  5.3903e-02 , runtime :  777 \n","step :  2900 , loss : 2.9316e-04 ,  Y0 :  5.3463e-02 , runtime :  803 \n","step :  3000 , loss : 2.8895e-04 ,  Y0 :  5.3260e-02 , runtime :  830 \n","step :  3100 , loss : 2.8752e-04 ,  Y0 :  5.3157e-02 , runtime :  858 \n","step :  3200 , loss : 2.8348e-04 ,  Y0 :  5.3238e-02 , runtime :  884 \n","step :  3300 , loss : 2.8070e-04 ,  Y0 :  5.3015e-02 , runtime :  913 \n","step :  3400 , loss : 2.7259e-04 ,  Y0 :  5.3012e-02 , runtime :  941 \n","step :  3500 , loss : 2.7038e-04 ,  Y0 :  5.2779e-02 , runtime :  968 \n","step :  3600 , loss : 2.6904e-04 ,  Y0 :  5.2549e-02 , runtime :  997 \n","step :  3700 , loss : 2.6775e-04 ,  Y0 :  5.2655e-02 , runtime : 1024 \n","step :  3800 , loss : 2.6483e-04 ,  Y0 :  5.2907e-02 , runtime : 1052 \n","step :  3900 , loss : 2.5856e-04 ,  Y0 :  5.3033e-02 , runtime : 1077 \n","step :  4000 , loss : 2.4918e-04 ,  Y0 :  5.3025e-02 , runtime : 1105 \n"," running time :  1105.734 s \n","3 run:\n"," Begin to solve Allen - Cahn equation \n","step :     0 , loss : 1.6503e-01 , Y0 :  5.4381e-01 , runtime :   15 \n","step :   100 , loss : 1.2897e-01 ,  Y0 :  4.9519e-01 , runtime :   47 \n","step :   200 , loss : 1.0115e-01 ,  Y0 :  4.5028e-01 , runtime :   75 \n","step :   300 , loss : 7.9315e-02 ,  Y0 :  4.0871e-01 , runtime :  105 \n","step :   400 , loss : 6.2281e-02 ,  Y0 :  3.7011e-01 , runtime :  134 \n","step :   500 , loss : 4.8510e-02 ,  Y0 :  3.3435e-01 , runtime :  162 \n","step :   600 , loss : 3.7477e-02 ,  Y0 :  3.0131e-01 , runtime :  190 \n","step :   700 , loss : 2.8558e-02 ,  Y0 :  2.7072e-01 , runtime :  219 \n","step :   800 , loss : 2.1568e-02 ,  Y0 :  2.4268e-01 , runtime :  247 \n","step :   900 , loss : 1.6143e-02 ,  Y0 :  2.1700e-01 , runtime :  272 \n","step :  1000 , loss : 1.1926e-02 ,  Y0 :  1.9363e-01 , runtime :  298 \n","step :  1100 , loss : 8.6391e-03 ,  Y0 :  1.7249e-01 , runtime :  328 \n","step :  1200 , loss : 6.1960e-03 ,  Y0 :  1.5363e-01 , runtime :  356 \n","step :  1300 , loss : 4.3937e-03 ,  Y0 :  1.3682e-01 , runtime :  385 \n","step :  1400 , loss : 3.0888e-03 ,  Y0 :  1.2204e-01 , runtime :  413 \n","step :  1500 , loss : 2.1652e-03 ,  Y0 :  1.0931e-01 , runtime :  442 \n","step :  1600 , loss : 1.5026e-03 ,  Y0 :  9.8287e-02 , runtime :  469 \n","step :  1700 , loss : 1.0580e-03 ,  Y0 :  8.8954e-02 , runtime :  495 \n","step :  1800 , loss : 7.5053e-04 ,  Y0 :  8.1072e-02 , runtime :  520 \n","step :  1900 , loss : 5.5559e-04 ,  Y0 :  7.4568e-02 , runtime :  550 \n","step :  2000 , loss : 4.3208e-04 ,  Y0 :  6.9401e-02 , runtime :  576 \n","step :  2100 , loss : 3.5805e-04 ,  Y0 :  6.5311e-02 , runtime :  603 \n","step :  2200 , loss : 3.0964e-04 ,  Y0 :  6.1969e-02 , runtime :  631 \n","step :  2300 , loss : 2.8353e-04 ,  Y0 :  5.9555e-02 , runtime :  657 \n","step :  2400 , loss : 2.6755e-04 ,  Y0 :  5.7557e-02 , runtime :  686 \n","step :  2500 , loss : 2.5704e-04 ,  Y0 :  5.6142e-02 , runtime :  711 \n","step :  2600 , loss : 2.5051e-04 ,  Y0 :  5.5089e-02 , runtime :  739 \n","step :  2700 , loss : 2.4902e-04 ,  Y0 :  5.4470e-02 , runtime :  768 \n","step :  2800 , loss : 2.4476e-04 ,  Y0 :  5.3898e-02 , runtime :  796 \n","step :  2900 , loss : 2.4146e-04 ,  Y0 :  5.3596e-02 , runtime :  823 \n","step :  3000 , loss : 2.3926e-04 ,  Y0 :  5.3344e-02 , runtime :  852 \n","step :  3100 , loss : 2.3670e-04 ,  Y0 :  5.3115e-02 , runtime :  880 \n","step :  3200 , loss : 2.3388e-04 ,  Y0 :  5.3172e-02 , runtime :  907 \n","step :  3300 , loss : 2.3157e-04 ,  Y0 :  5.2963e-02 , runtime :  934 \n","step :  3400 , loss : 2.2751e-04 ,  Y0 :  5.2909e-02 , runtime :  961 \n","step :  3500 , loss : 2.2928e-04 ,  Y0 :  5.2796e-02 , runtime :  990 \n","step :  3600 , loss : 2.2354e-04 ,  Y0 :  5.2898e-02 , runtime : 1019 \n","step :  3700 , loss : 2.1863e-04 ,  Y0 :  5.2739e-02 , runtime : 1045 \n","step :  3800 , loss : 2.1691e-04 ,  Y0 :  5.2880e-02 , runtime : 1073 \n","step :  3900 , loss : 2.1234e-04 ,  Y0 :  5.2745e-02 , runtime : 1100 \n","step :  4000 , loss : 2.1018e-04 ,  Y0 :  5.2705e-02 , runtime : 1125 \n"," running time :  1125.924 s \n","4 run:\n"," Begin to solve Allen - Cahn equation \n","step :     0 , loss : 1.5728e-01 , Y0 :  5.4381e-01 , runtime :   17 \n","step :   100 , loss : 1.2293e-01 ,  Y0 :  4.9526e-01 , runtime :   49 \n","step :   200 , loss : 9.6782e-02 ,  Y0 :  4.5025e-01 , runtime :   76 \n","step :   300 , loss : 7.6225e-02 ,  Y0 :  4.0867e-01 , runtime :  104 \n","step :   400 , loss : 5.9559e-02 ,  Y0 :  3.7022e-01 , runtime :  134 \n","step :   500 , loss : 4.6140e-02 ,  Y0 :  3.3448e-01 , runtime :  159 \n","step :   600 , loss : 3.5656e-02 ,  Y0 :  3.0149e-01 , runtime :  186 \n","step :   700 , loss : 2.7164e-02 ,  Y0 :  2.7099e-01 , runtime :  215 \n","step :   800 , loss : 2.0474e-02 ,  Y0 :  2.4288e-01 , runtime :  242 \n","step :   900 , loss : 1.5317e-02 ,  Y0 :  2.1719e-01 , runtime :  271 \n","step :  1000 , loss : 1.1282e-02 ,  Y0 :  1.9379e-01 , runtime :  298 \n","step :  1100 , loss : 8.1947e-03 ,  Y0 :  1.7269e-01 , runtime :  326 \n","step :  1200 , loss : 5.8976e-03 ,  Y0 :  1.5376e-01 , runtime :  356 \n","step :  1300 , loss : 4.1677e-03 ,  Y0 :  1.3706e-01 , runtime :  384 \n","step :  1400 , loss : 2.9126e-03 ,  Y0 :  1.2233e-01 , runtime :  412 \n","step :  1500 , loss : 2.0146e-03 ,  Y0 :  1.0942e-01 , runtime :  439 \n","step :  1600 , loss : 1.3906e-03 ,  Y0 :  9.8355e-02 , runtime :  469 \n","step :  1700 , loss : 9.7538e-04 ,  Y0 :  8.9111e-02 , runtime :  495 \n","step :  1800 , loss : 7.0294e-04 ,  Y0 :  8.1287e-02 , runtime :  522 \n","step :  1900 , loss : 5.1893e-04 ,  Y0 :  7.4731e-02 , runtime :  549 \n","step :  2000 , loss : 4.0589e-04 ,  Y0 :  6.9412e-02 , runtime :  578 \n","step :  2100 , loss : 3.3779e-04 ,  Y0 :  6.5235e-02 , runtime :  607 \n","step :  2200 , loss : 2.9562e-04 ,  Y0 :  6.1906e-02 , runtime :  635 \n","step :  2300 , loss : 2.7100e-04 ,  Y0 :  5.9350e-02 , runtime :  660 \n","step :  2400 , loss : 2.5618e-04 ,  Y0 :  5.7465e-02 , runtime :  687 \n","step :  2500 , loss : 2.4929e-04 ,  Y0 :  5.6044e-02 , runtime :  715 \n","step :  2600 , loss : 2.4826e-04 ,  Y0 :  5.4980e-02 , runtime :  742 \n","step :  2700 , loss : 2.4376e-04 ,  Y0 :  5.4240e-02 , runtime :  769 \n","step :  2800 , loss : 2.4170e-04 ,  Y0 :  5.3927e-02 , runtime :  799 \n","step :  2900 , loss : 2.3710e-04 ,  Y0 :  5.3612e-02 , runtime :  827 \n","step :  3000 , loss : 2.3303e-04 ,  Y0 :  5.3446e-02 , runtime :  855 \n","step :  3100 , loss : 2.3012e-04 ,  Y0 :  5.3174e-02 , runtime :  881 \n","step :  3200 , loss : 2.2687e-04 ,  Y0 :  5.3117e-02 , runtime :  907 \n","step :  3300 , loss : 2.2361e-04 ,  Y0 :  5.3129e-02 , runtime :  935 \n","step :  3400 , loss : 2.2253e-04 ,  Y0 :  5.3009e-02 , runtime :  963 \n","step :  3500 , loss : 2.2289e-04 ,  Y0 :  5.3020e-02 , runtime :  991 \n","step :  3600 , loss : 2.2251e-04 ,  Y0 :  5.3136e-02 , runtime : 1019 \n","step :  3700 , loss : 2.1915e-04 ,  Y0 :  5.2960e-02 , runtime : 1048 \n","step :  3800 , loss : 2.1770e-04 ,  Y0 :  5.2823e-02 , runtime : 1075 \n","step :  3900 , loss : 2.1391e-04 ,  Y0 :  5.2791e-02 , runtime : 1102 \n","step :  4000 , loss : 2.0840e-04 ,  Y0 :  5.2983e-02 , runtime : 1131 \n"," running time :  1131.582 s \n"]}],"source":["import time\n","import math\n","import tensorflow.compat.v1 as tf\n","import numpy as np\n","from tensorflow.python.training.moving_averages import assign_moving_average\n","from scipy.stats import multivariate_normal as normal            # Generate normally distributed random numbers\n","from tensorflow import random_normal_initializer as norm_init    #Initializers for generating tensors with normal distributions\n","from tensorflow import random_uniform_initializer as unif_init   #Generating initializers with uniformly distributed tensors\n","from tensorflow import constant_initializer as const_init        #Generating initializers for tensors with constant values\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()\n","\n","class SolveAllenCahn (object):\n","    \"\"\" The fully-connected neural network model .\"\"\"\n","    def __init__ (self,sess):\n","        self.sess = sess\n","        # parameters for the PDE\n","        self.d = 100      # Dimensions of data\n","        self.T = 0.3      # Length of time for each path\n","        # parameters for the algorithm\n","        self.n_time = 20     # Composed of 20 networks\n","        self.n_layer = 4     # Number of layers of the neural network\n","        self.n_neuron = [self.d ,self.d +10 ,self.d +10, self.d ]    # Number of neurons in each layer, corresponding to input, hidden layer 1, hidden layer 2, output\n","        self.batch_size = 64      # Used one at a time for path calculation, 64*4=256\n","        self.valid_size = 256     # 256 Monte Carlo samples (paths)\n","        self.n_maxstep = 4000     #Iteration steps\n","        self.n_displaystep = 100     # Print every 100 steps\n","        self.learning_rate = 5e-4\n","        self.Yini = [0.3,0.6]       # Maximum and minimum of initial value Y0\n","        # some basic constants and variables\n","        self.h = (self.T +0.0)/self.n_time    # Interval of data in each path，delta t\n","        self.sqrth = math.sqrt(self.h)\n","        self.t_stamp = np.arange(0,self.n_time)*self.h  # Timestamp, accumulated time\n","        self._extra_train_ops = []  # batch moving average operation, which requires additional training of beta and gamma\n","\n","    def train(self):\n","        # For training of neural networks\n","        start_time = time.time()\n","        # train operations create new tensorflow variable,name='global_step' with yield generator\n","        self.global_step = \\\n","            tf.get_variable('global_step', [] ,\n","                              initializer = tf.constant_initializer(1),\n","                              trainable = False,dtype = tf.int32 )\n","        trainable_vars = tf.trainable_variables()\n","        grads = tf.gradients(self.loss,trainable_vars)\n","        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n","        apply_op = \\\n","            optimizer.apply_gradients(zip(grads,trainable_vars) ,\n","                                          global_step = self.global_step)\n","\n","        train_ops = [apply_op] + self._extra_train_ops\n","        self.train_op = tf.group(* train_ops)\n","\n","        self.loss_history = []   # Used to record loss values\n","        self.init_history = []   # Used to record the value of Y0\n","        self.run_time = []\n","\n","        # for validation\n","        dW_valid , X_valid = self.sample_path(self.valid_size)\n","        feed_dict_valid = { self.dW : dW_valid,\n","                            self.X : X_valid,\n","                            self.is_training: False }\n","        # initialization\n","        step = 1\n","        self.sess.run (tf.global_variables_initializer())\n","\n","        # Operational framework\n","        temp_loss = self.sess.run(self.loss ,\n","                                  feed_dict = feed_dict_valid )\n","\n","        temp_init = self.Y0.eval()[0]\n","        self.loss_history.append(temp_loss)\n","        self.init_history.append(temp_init)\n","        self.run_time.append(time.time()-start_time + self.t_bd)\n","        print(\"step : %5u , loss : %.4e , \" % \\\n","                (0 ,temp_loss ) + \\\n","                \"Y0 : % .4e , runtime : %4u \" % \\\n","                (temp_init, time.time()-start_time + self.t_bd))\n","\n","        # begin sgd iteration，0-4000STEP\n","        for _ in range (self.n_maxstep +1):\n","            step = self.sess.run (self.global_step)\n","            dW_train,X_train = self.sample_path(self.batch_size)\n","            self.sess.run(self.train_op,\n","                          feed_dict ={self.dW : dW_train ,\n","                                      self.X : X_train ,\n","                                      self.is_training : True })\n","            if step % self.n_displaystep == 0:   # Test the loss and the value of Y0 every 100 steps with the validation set\n","                temp_loss = self.sess.run(self.loss ,\n","                                          feed_dict = feed_dict_valid)\n","                temp_init = self.Y0.eval()[0]\n","                self.loss_history.append(temp_loss)\n","                self.init_history.append(temp_init )\n","                self.run_time.append(time.time()-start_time + self.t_bd)\n","                print(\"step : % 5u , loss : %.4e , \" % \\\n","                        ( step , temp_loss ) + \\\n","                        \" Y0 : % .4e , runtime : %4u \" % \\\n","                        (temp_init , time.time() - start_time + self.t_bd ))\n","            step += 1\n","        end_time = time.time()\n","        print(\" running time : % .3f s \" % \\\n","                ( end_time - start_time + self.t_bd ))\n","\n","    def build(self):\n","        # build the whole network by stacking subnetworks\n","        start_time = time.time ()\n","        # dW、X、is_training placeholder\n","        self.dW = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time ] ,name = 'dW')   # None*100*20\n","        self.X = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time +1] ,name = 'X')   # None*100*20\n","        self.is_training = tf.placeholder (tf.bool)\n","\n","        # Initialize Y0\\Z0\n","        self.Y0 = tf.Variable(tf.random_uniform([1] ,                      # u0\n","                                                minval = self.Yini [0] ,   # MIN0.3\n","                                                maxval = self.Yini [1] ,   # MAX0.6\n","                                                dtype = tf.float32 ));\n","        self.Z0 = tf.Variable (tf.random_uniform ([1,self.d] ,    # The initial value of the u-gradient, a 1*d vector\n","                                                minval = -.1 ,\n","                                                maxval =.1 ,\n","                                                dtype = tf.float32 ))\n","        self.allones = \\\n","             tf.ones(shape = tf.stack([ tf.shape(self.dW)[0],1]) ,   # tf.shape(self.dW)[0]=len(batch),shape=(batch,1)\n","                         dtype = tf.float32 )\n","\n","        Y = self.allones * self.Y0\n","        Z = tf.matmul(self.allones, self.Z0 )\n","\n","\n","        with tf.variable_scope('forward'):\n","            for t in range(0,self.n_time -1):\n","\n","                    Y = Y - self.f_tf(self.t_stamp[t] ,\n","                                    self.X[:,:,t],Y,Z)* self.h\n","                    Y = Y + tf.reduce_sum(Z * self.dW[:,:,t],1,\n","                                       keep_dims = True )\n","                    Z = self._one_time_net(self.X[:,:,t +1] ,\n","                                       str(t +1))/self.d\n","            # terminal time\n","            Y = Y - self.f_tf(self.t_stamp[self.n_time -1] ,\n","                                  self.X[:,:,self.n_time -1] ,\n","                                  Y,Z)* self.h\n","            Y = Y + tf.reduce_sum(Z * self.dW [:,:,self.n_time -1] , 1 ,\n","                                      keep_dims = True )\n","            term_delta = Y - self.g_tf(self.T,\n","                                   self.X[:,:,self.n_time])\n","            self.clipped_delta = \\\n","                  tf.clip_by_value(term_delta ,-50.0 , 50.0)\n","            self.loss = tf.reduce_mean(self.clipped_delta**2)\n","        self.t_bd = time.time() - start_time\n","\n","    def sample_path(self, n_sample):\n","        # （xt,(wt-wt-1)）\n","        dW_sample = np.zeros([n_sample,self.d,self.n_time])\n","        X_sample = np.zeros([n_sample,self.d,self.n_time +1])\n","        for i in range(self.n_time):\n","            dW_sample [:,:,i] = \\\n","               np.reshape(normal.rvs(mean = np.zeros(self.d) ,\n","                                     cov =1 ,\n","                                     size = n_sample)* self.sqrth ,\n","                          (n_sample,self.d))\n","            X_sample[:,:,i +1] = X_sample[:,:,i] + \\\n","                                   np.sqrt(2)*dW_sample[:,:,i]\n","        return dW_sample, X_sample\n","\n","    def f_tf(self,t,X,Y,Z ):\n","        # nonlinear term\n","        return Y - tf.pow(Y,3)\n","\n","    def g_tf(self,t,X):\n","        # terminal conditions\n","        return 0.5/(1 + 0.2* tf.reduce_sum(X **2,1,keep_dims = True ))\n","\n","    def _one_time_net(self , x ,name ):\n","\n","        with tf.variable_scope(name):\n","            x_norm = self._batch_norm(x , name = 'layer0_normal')  # Normalize batch as input\n","            layer1 = self._one_layer(x_norm , self.n_neuron [1] ,   # Hidden Layer 1 input(batch,d),output(batch，d+10)\n","                                      name = 'layer1')\n","            layer2 = self._one_layer(layer1,self.n_neuron[2] ,  # Hidden Layer 2 input(batch,d+10),output(batch,d+10)\n","                                      name = 'layer2')\n","            z = self._one_layer(layer2 , self.n_neuron [3] , #  input(batch,d+10),output(baatch,d)\n","                                     activation_fn = None , name = 'final')\n","        return z\n","\n","    def _one_layer(self , input_ , out_sz ,\n","                   activation_fn = tf.nn.relu ,\n","                   std =5.0 , name = 'linear'):\n","\n","        with tf.variable_scope(name):\n","            shape = input_.get_shape().as_list()\n","            w = tf.get_variable('Matrix',\n","                                [shape[1], out_sz] ,tf.float32,\n","                                norm_init(stddev = \\\n","                                          std / np.sqrt(shape[1]+ out_sz )))\n","            hidden = tf.matmul(input_ ,w )\n","            hidden_bn = self._batch_norm(hidden, name = 'normal')\n","        if activation_fn != None :\n","            return activation_fn(hidden_bn)\n","        else :\n","            return hidden_bn\n","\n","    def _batch_norm(self , x , name ):\n","        \"\"\" Batch normalization \"\"\"\n","        with tf.variable_scope(name):\n","            params_shape = [x.get_shape()[ -1]]   # [d,d+10,d+10,d]\n","            beta = tf.get_variable('beta', params_shape ,\n","                                         tf.float32 ,\n","                                         norm_init(0.0 , stddev =0.1 ,\n","                                         ))\n","            gamma = tf.get_variable( 'gamma', params_shape ,\n","                                         tf.float32 ,\n","                                         unif_init (0.1,0.5 ,\n","                                          ))\n","            mv_mean = tf.get_variable('moving_mean' ,\n","                                         params_shape ,\n","                                         tf.float32 ,\n","                                         const_init (0.0) ,\n","                                         trainable = False )\n","            mv_var = tf.get_variable('moving_variance' ,\n","                                        params_shape ,\n","                                        tf.float32 ,\n","                                        const_init(1.0) ,\n","                                        trainable = False )\n","\n","            # These ops will only be preformed when training\n","            mean ,variance = tf.nn.moments(x ,[0] , name = 'moments')\n","            self._extra_train_ops.append (\\\n","                 assign_moving_average(mv_mean , mean , 0.99))\n","            self._extra_train_ops.append (\\\n","                 assign_moving_average(mv_var , variance , 0.99))\n","\n","            mean,variance = \\\n","                tf.cond(self.is_training ,            # control_flow_ops.cond Controls the execution flow, with the first being a condition\n","                                     lambda :( mean , variance ) ,\n","                                     lambda :( mv_mean , mv_var ))\n","\n","            y = tf.nn.batch_normalization (x , mean , variance ,\n","                                           beta , gamma , 1e-6)\n","\n","            y.set_shape( x.get_shape())\n","            return y\n","\n","def main (name):\n","    tf.reset_default_graph ()\n","    with tf.Session() as sess :\n","        tf.set_random_seed (1)\n","        print(\" Begin to solve Allen - Cahn equation \")\n","        model = SolveAllenCahn (sess)\n","        model.build()\n","        model.train ()\n","        output = np.zeros ((len(model.init_history), 4))\n","        output[:,0] = np.arange(len( model.init_history )) \\\n","                             * model.n_displaystep\n","        output[:,1] = model.loss_history\n","        output[:,2] = model.init_history\n","        output[:,3] = model.run_time\n","        np.savetxt(\"./ AllenCahn_d100\"+str(name)+\".csv \" ,  # Saving the output\n","                     output ,\n","                     fmt =[ '%d', '%.5e', '%.5e','%d'] ,\n","                     delimiter =\",\",\n","                     header =\"step ,loss function , \" + \\\n","                     \" target value , runtime \" ,\n","                     comments = '')\n","\n","if __name__ == '__main__':\n","        np.random.seed(1) # Define a random number seed\n","        for i in range(5):\n","            print(str(i)+' run:')\n","            main(i)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[{"file_id":"1REeLrKxJXryTh54hjUAy1dSxm_OuePBR","timestamp":1728521767207}]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.1"}},"nbformat":4,"nbformat_minor":0}
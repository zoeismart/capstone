{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZkEQbs8AJwWH"
   },
   "source": [
    "##### Deep Learning to Solve Higher Differential Equations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iIDPN9CBJwWP",
    "outputId": "16a58b69-5027-4357-902e-9e9ba03d7a4d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:108: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 run:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin to solve Allen - Cahn equation\n",
      "End build\n",
      "step :     0 , loss : 1.6257e-01 , Y0 :  5.4381e-01 , runtime :    3 \n",
      "step :   100 , loss : 1.2720e-01 ,  Y0 :  4.9521e-01 , runtime :   13 \n",
      "step :   200 , loss : 9.9606e-02 ,  Y0 :  4.5011e-01 , runtime :   22 \n",
      "step :   300 , loss : 7.8281e-02 ,  Y0 :  4.0847e-01 , runtime :   31 \n",
      "step :   400 , loss : 6.0937e-02 ,  Y0 :  3.6993e-01 , runtime :   40 \n",
      "step :   500 , loss : 4.7326e-02 ,  Y0 :  3.3413e-01 , runtime :   48 \n",
      "step :   600 , loss : 3.6222e-02 ,  Y0 :  3.0113e-01 , runtime :   57 \n",
      "step :   700 , loss : 2.7688e-02 ,  Y0 :  2.7057e-01 , runtime :   65 \n",
      "step :   800 , loss : 2.0818e-02 ,  Y0 :  2.4244e-01 , runtime :   74 \n",
      "step :   900 , loss : 1.5454e-02 ,  Y0 :  2.1669e-01 , runtime :   82 \n",
      "step :  1000 , loss : 1.1271e-02 ,  Y0 :  1.9334e-01 , runtime :   90 \n",
      "step :  1100 , loss : 8.1346e-03 ,  Y0 :  1.7226e-01 , runtime :   99 \n",
      "step :  1200 , loss : 5.7580e-03 ,  Y0 :  1.5352e-01 , runtime :  107 \n",
      "step :  1300 , loss : 4.0163e-03 ,  Y0 :  1.3678e-01 , runtime :  116 \n",
      "step :  1400 , loss : 2.7328e-03 ,  Y0 :  1.2203e-01 , runtime :  125 \n",
      "step :  1500 , loss : 1.8390e-03 ,  Y0 :  1.0923e-01 , runtime :  133 \n",
      "step :  1600 , loss : 1.1990e-03 ,  Y0 :  9.8166e-02 , runtime :  142 \n",
      "step :  1700 , loss : 7.6543e-04 ,  Y0 :  8.8806e-02 , runtime :  151 \n",
      "step :  1800 , loss : 4.8272e-04 ,  Y0 :  8.0997e-02 , runtime :  159 \n",
      "step :  1900 , loss : 2.9970e-04 ,  Y0 :  7.4594e-02 , runtime :  168 \n",
      "step :  2000 , loss : 1.8402e-04 ,  Y0 :  6.9412e-02 , runtime :  177 \n",
      "step :  2100 , loss : 1.1680e-04 ,  Y0 :  6.5302e-02 , runtime :  186 \n",
      "step :  2200 , loss : 7.6327e-05 ,  Y0 :  6.2005e-02 , runtime :  194 \n",
      "step :  2300 , loss : 5.3234e-05 ,  Y0 :  5.9485e-02 , runtime :  203 \n",
      "step :  2400 , loss : 4.0355e-05 ,  Y0 :  5.7561e-02 , runtime :  212 \n",
      "step :  2500 , loss : 3.3557e-05 ,  Y0 :  5.6139e-02 , runtime :  221 \n",
      "step :  2600 , loss : 3.0296e-05 ,  Y0 :  5.5104e-02 , runtime :  230 \n",
      "step :  2700 , loss : 2.8941e-05 ,  Y0 :  5.4393e-02 , runtime :  238 \n",
      "step :  2800 , loss : 2.8151e-05 ,  Y0 :  5.3884e-02 , runtime :  247 \n",
      "step :  2900 , loss : 2.7637e-05 ,  Y0 :  5.3533e-02 , runtime :  255 \n",
      "step :  3000 , loss : 2.7518e-05 ,  Y0 :  5.3269e-02 , runtime :  264 \n",
      "step :  3100 , loss : 2.7225e-05 ,  Y0 :  5.3161e-02 , runtime :  273 \n",
      "step :  3200 , loss : 2.6911e-05 ,  Y0 :  5.3066e-02 , runtime :  282 \n",
      "step :  3300 , loss : 2.7105e-05 ,  Y0 :  5.2983e-02 , runtime :  290 \n",
      "step :  3400 , loss : 2.7383e-05 ,  Y0 :  5.2909e-02 , runtime :  299 \n",
      "step :  3500 , loss : 2.7745e-05 ,  Y0 :  5.2919e-02 , runtime :  307 \n",
      "step :  3600 , loss : 2.7639e-05 ,  Y0 :  5.2884e-02 , runtime :  316 \n",
      "step :  3700 , loss : 2.7331e-05 ,  Y0 :  5.2905e-02 , runtime :  324 \n",
      "step :  3800 , loss : 2.7727e-05 ,  Y0 :  5.2850e-02 , runtime :  333 \n",
      "step :  3900 , loss : 2.7446e-05 ,  Y0 :  5.2876e-02 , runtime :  341 \n",
      "step :  4000 , loss : 2.7105e-05 ,  Y0 :  5.2907e-02 , runtime :  350 \n",
      " running time :  350.409 s \n",
      "1 run:\n",
      "Begin to solve Allen - Cahn equation\n",
      "End build\n",
      "step :     0 , loss : 1.5893e-01 , Y0 :  5.4381e-01 , runtime :    3 \n",
      "step :   100 , loss : 1.2470e-01 ,  Y0 :  4.9533e-01 , runtime :   13 \n",
      "step :   200 , loss : 9.7599e-02 ,  Y0 :  4.5045e-01 , runtime :   20 \n",
      "step :   300 , loss : 7.6726e-02 ,  Y0 :  4.0884e-01 , runtime :   29 \n",
      "step :   400 , loss : 5.9759e-02 ,  Y0 :  3.7018e-01 , runtime :   38 \n",
      "step :   500 , loss : 4.6334e-02 ,  Y0 :  3.3435e-01 , runtime :   46 \n",
      "step :   600 , loss : 3.5546e-02 ,  Y0 :  3.0124e-01 , runtime :   55 \n",
      "step :   700 , loss : 2.7199e-02 ,  Y0 :  2.7076e-01 , runtime :   64 \n",
      "step :   800 , loss : 2.0504e-02 ,  Y0 :  2.4269e-01 , runtime :   72 \n",
      "step :   900 , loss : 1.5314e-02 ,  Y0 :  2.1705e-01 , runtime :   80 \n",
      "step :  1000 , loss : 1.1311e-02 ,  Y0 :  1.9380e-01 , runtime :   89 \n",
      "step :  1100 , loss : 8.1225e-03 ,  Y0 :  1.7267e-01 , runtime :   98 \n",
      "step :  1200 , loss : 5.7492e-03 ,  Y0 :  1.5385e-01 , runtime :  106 \n",
      "step :  1300 , loss : 4.0055e-03 ,  Y0 :  1.3709e-01 , runtime :  115 \n",
      "step :  1400 , loss : 2.7335e-03 ,  Y0 :  1.2235e-01 , runtime :  123 \n",
      "step :  1500 , loss : 1.8275e-03 ,  Y0 :  1.0952e-01 , runtime :  132 \n",
      "step :  1600 , loss : 1.1960e-03 ,  Y0 :  9.8435e-02 , runtime :  141 \n",
      "step :  1700 , loss : 7.6010e-04 ,  Y0 :  8.9021e-02 , runtime :  149 \n",
      "step :  1800 , loss : 4.8267e-04 ,  Y0 :  8.1210e-02 , runtime :  158 \n",
      "step :  1900 , loss : 2.9967e-04 ,  Y0 :  7.4756e-02 , runtime :  167 \n",
      "step :  2000 , loss : 1.8311e-04 ,  Y0 :  6.9514e-02 , runtime :  176 \n",
      "step :  2100 , loss : 1.1405e-04 ,  Y0 :  6.5294e-02 , runtime :  185 \n",
      "step :  2200 , loss : 7.4454e-05 ,  Y0 :  6.1975e-02 , runtime :  193 \n",
      "step :  2300 , loss : 5.2883e-05 ,  Y0 :  5.9445e-02 , runtime :  202 \n",
      "step :  2400 , loss : 4.0532e-05 ,  Y0 :  5.7496e-02 , runtime :  211 \n",
      "step :  2500 , loss : 3.4691e-05 ,  Y0 :  5.6067e-02 , runtime :  220 \n",
      "step :  2600 , loss : 3.1983e-05 ,  Y0 :  5.5043e-02 , runtime :  229 \n",
      "step :  2700 , loss : 3.0748e-05 ,  Y0 :  5.4304e-02 , runtime :  238 \n",
      "step :  2800 , loss : 3.0329e-05 ,  Y0 :  5.3807e-02 , runtime :  246 \n",
      "step :  2900 , loss : 2.9857e-05 ,  Y0 :  5.3494e-02 , runtime :  255 \n",
      "step :  3000 , loss : 2.9416e-05 ,  Y0 :  5.3265e-02 , runtime :  264 \n",
      "step :  3100 , loss : 2.9569e-05 ,  Y0 :  5.3066e-02 , runtime :  273 \n",
      "step :  3200 , loss : 2.9719e-05 ,  Y0 :  5.2965e-02 , runtime :  282 \n",
      "step :  3300 , loss : 2.9424e-05 ,  Y0 :  5.2940e-02 , runtime :  290 \n",
      "step :  3400 , loss : 2.9895e-05 ,  Y0 :  5.2923e-02 , runtime :  299 \n",
      "step :  3500 , loss : 2.9864e-05 ,  Y0 :  5.2868e-02 , runtime :  308 \n",
      "step :  3600 , loss : 3.0221e-05 ,  Y0 :  5.2901e-02 , runtime :  317 \n",
      "step :  3700 , loss : 2.9951e-05 ,  Y0 :  5.2903e-02 , runtime :  326 \n",
      "step :  3800 , loss : 3.0061e-05 ,  Y0 :  5.2808e-02 , runtime :  335 \n",
      "step :  3900 , loss : 3.0357e-05 ,  Y0 :  5.2806e-02 , runtime :  343 \n",
      "step :  4000 , loss : 3.0289e-05 ,  Y0 :  5.2918e-02 , runtime :  352 \n",
      " running time :  352.906 s \n",
      "2 run:\n",
      "Begin to solve Allen - Cahn equation\n",
      "End build\n",
      "step :     0 , loss : 1.6381e-01 , Y0 :  5.4381e-01 , runtime :    2 \n",
      "step :   100 , loss : 1.2826e-01 ,  Y0 :  4.9519e-01 , runtime :   12 \n",
      "step :   200 , loss : 1.0075e-01 ,  Y0 :  4.5034e-01 , runtime :   21 \n",
      "step :   300 , loss : 7.9387e-02 ,  Y0 :  4.0878e-01 , runtime :   30 \n",
      "step :   400 , loss : 6.1818e-02 ,  Y0 :  3.7007e-01 , runtime :   39 \n",
      "step :   500 , loss : 4.7630e-02 ,  Y0 :  3.3427e-01 , runtime :   48 \n",
      "step :   600 , loss : 3.6430e-02 ,  Y0 :  3.0109e-01 , runtime :   57 \n",
      "step :   700 , loss : 2.7867e-02 ,  Y0 :  2.7058e-01 , runtime :   65 \n",
      "step :   800 , loss : 2.0898e-02 ,  Y0 :  2.4248e-01 , runtime :   74 \n",
      "step :   900 , loss : 1.5575e-02 ,  Y0 :  2.1684e-01 , runtime :   83 \n",
      "step :  1000 , loss : 1.1390e-02 ,  Y0 :  1.9346e-01 , runtime :   92 \n",
      "step :  1100 , loss : 8.2220e-03 ,  Y0 :  1.7226e-01 , runtime :  101 \n",
      "step :  1200 , loss : 5.8399e-03 ,  Y0 :  1.5341e-01 , runtime :  110 \n",
      "step :  1300 , loss : 4.0330e-03 ,  Y0 :  1.3669e-01 , runtime :  119 \n",
      "step :  1400 , loss : 2.7271e-03 ,  Y0 :  1.2198e-01 , runtime :  128 \n",
      "step :  1500 , loss : 1.8275e-03 ,  Y0 :  1.0925e-01 , runtime :  136 \n",
      "step :  1600 , loss : 1.1912e-03 ,  Y0 :  9.8257e-02 , runtime :  145 \n",
      "step :  1700 , loss : 7.5349e-04 ,  Y0 :  8.8945e-02 , runtime :  154 \n",
      "step :  1800 , loss : 4.6664e-04 ,  Y0 :  8.1071e-02 , runtime :  163 \n",
      "step :  1900 , loss : 2.9158e-04 ,  Y0 :  7.4640e-02 , runtime :  172 \n",
      "step :  2000 , loss : 1.8051e-04 ,  Y0 :  6.9443e-02 , runtime :  181 \n",
      "step :  2100 , loss : 1.1442e-04 ,  Y0 :  6.5288e-02 , runtime :  190 \n",
      "step :  2200 , loss : 7.6313e-05 ,  Y0 :  6.2007e-02 , runtime :  198 \n",
      "step :  2300 , loss : 5.4102e-05 ,  Y0 :  5.9455e-02 , runtime :  207 \n",
      "step :  2400 , loss : 4.2951e-05 ,  Y0 :  5.7529e-02 , runtime :  216 \n",
      "step :  2500 , loss : 3.7007e-05 ,  Y0 :  5.6131e-02 , runtime :  225 \n",
      "step :  2600 , loss : 3.3922e-05 ,  Y0 :  5.5096e-02 , runtime :  234 \n",
      "step :  2700 , loss : 3.2223e-05 ,  Y0 :  5.4426e-02 , runtime :  242 \n",
      "step :  2800 , loss : 3.1502e-05 ,  Y0 :  5.3917e-02 , runtime :  251 \n",
      "step :  2900 , loss : 3.1087e-05 ,  Y0 :  5.3509e-02 , runtime :  260 \n",
      "step :  3000 , loss : 3.0466e-05 ,  Y0 :  5.3306e-02 , runtime :  269 \n",
      "step :  3100 , loss : 3.0453e-05 ,  Y0 :  5.3141e-02 , runtime :  277 \n",
      "step :  3200 , loss : 3.0464e-05 ,  Y0 :  5.3074e-02 , runtime :  286 \n",
      "step :  3300 , loss : 3.0089e-05 ,  Y0 :  5.2986e-02 , runtime :  294 \n",
      "step :  3400 , loss : 3.0327e-05 ,  Y0 :  5.2929e-02 , runtime :  302 \n",
      "step :  3500 , loss : 3.0521e-05 ,  Y0 :  5.2925e-02 , runtime :  311 \n",
      "step :  3600 , loss : 3.0387e-05 ,  Y0 :  5.2886e-02 , runtime :  319 \n",
      "step :  3700 , loss : 3.0165e-05 ,  Y0 :  5.2897e-02 , runtime :  328 \n",
      "step :  3800 , loss : 2.9911e-05 ,  Y0 :  5.2906e-02 , runtime :  337 \n",
      "step :  3900 , loss : 2.9683e-05 ,  Y0 :  5.2860e-02 , runtime :  345 \n",
      "step :  4000 , loss : 2.9176e-05 ,  Y0 :  5.2892e-02 , runtime :  353 \n",
      " running time :  353.679 s \n",
      "3 run:\n",
      "Begin to solve Allen - Cahn equation\n",
      "End build\n",
      "step :     0 , loss : 1.6538e-01 , Y0 :  5.4381e-01 , runtime :    2 \n",
      "step :   100 , loss : 1.3039e-01 ,  Y0 :  4.9519e-01 , runtime :   12 \n",
      "step :   200 , loss : 1.0230e-01 ,  Y0 :  4.5030e-01 , runtime :   21 \n",
      "step :   300 , loss : 8.0293e-02 ,  Y0 :  4.0879e-01 , runtime :   30 \n",
      "step :   400 , loss : 6.3093e-02 ,  Y0 :  3.7020e-01 , runtime :   39 \n",
      "step :   500 , loss : 4.9134e-02 ,  Y0 :  3.3438e-01 , runtime :   47 \n",
      "step :   600 , loss : 3.8023e-02 ,  Y0 :  3.0136e-01 , runtime :   56 \n",
      "step :   700 , loss : 2.8913e-02 ,  Y0 :  2.7078e-01 , runtime :   65 \n",
      "step :   800 , loss : 2.1855e-02 ,  Y0 :  2.4274e-01 , runtime :   74 \n",
      "step :   900 , loss : 1.6235e-02 ,  Y0 :  2.1706e-01 , runtime :   83 \n",
      "step :  1000 , loss : 1.1962e-02 ,  Y0 :  1.9369e-01 , runtime :   92 \n",
      "step :  1100 , loss : 8.6232e-03 ,  Y0 :  1.7256e-01 , runtime :  101 \n",
      "step :  1200 , loss : 6.1269e-03 ,  Y0 :  1.5366e-01 , runtime :  109 \n",
      "step :  1300 , loss : 4.2792e-03 ,  Y0 :  1.3690e-01 , runtime :  118 \n",
      "step :  1400 , loss : 2.9486e-03 ,  Y0 :  1.2217e-01 , runtime :  127 \n",
      "step :  1500 , loss : 2.0004e-03 ,  Y0 :  1.0940e-01 , runtime :  136 \n",
      "step :  1600 , loss : 1.3152e-03 ,  Y0 :  9.8354e-02 , runtime :  145 \n",
      "step :  1700 , loss : 8.5204e-04 ,  Y0 :  8.8981e-02 , runtime :  153 \n",
      "step :  1800 , loss : 5.3644e-04 ,  Y0 :  8.1112e-02 , runtime :  162 \n",
      "step :  1900 , loss : 3.3703e-04 ,  Y0 :  7.4651e-02 , runtime :  171 \n",
      "step :  2000 , loss : 2.0888e-04 ,  Y0 :  6.9437e-02 , runtime :  180 \n",
      "step :  2100 , loss : 1.3188e-04 ,  Y0 :  6.5251e-02 , runtime :  188 \n",
      "step :  2200 , loss : 8.6844e-05 ,  Y0 :  6.1973e-02 , runtime :  197 \n",
      "step :  2300 , loss : 5.9717e-05 ,  Y0 :  5.9451e-02 , runtime :  206 \n",
      "step :  2400 , loss : 4.5879e-05 ,  Y0 :  5.7561e-02 , runtime :  215 \n",
      "step :  2500 , loss : 3.6980e-05 ,  Y0 :  5.6172e-02 , runtime :  224 \n",
      "step :  2600 , loss : 3.2940e-05 ,  Y0 :  5.5142e-02 , runtime :  232 \n",
      "step :  2700 , loss : 3.0663e-05 ,  Y0 :  5.4418e-02 , runtime :  241 \n",
      "step :  2800 , loss : 2.8912e-05 ,  Y0 :  5.3881e-02 , runtime :  250 \n",
      "step :  2900 , loss : 2.8690e-05 ,  Y0 :  5.3570e-02 , runtime :  259 \n",
      "step :  3000 , loss : 2.7882e-05 ,  Y0 :  5.3238e-02 , runtime :  267 \n",
      "step :  3100 , loss : 2.7427e-05 ,  Y0 :  5.3081e-02 , runtime :  276 \n",
      "step :  3200 , loss : 2.7211e-05 ,  Y0 :  5.3076e-02 , runtime :  285 \n",
      "step :  3300 , loss : 2.7355e-05 ,  Y0 :  5.3040e-02 , runtime :  294 \n",
      "step :  3400 , loss : 2.7210e-05 ,  Y0 :  5.2996e-02 , runtime :  303 \n",
      "step :  3500 , loss : 2.6984e-05 ,  Y0 :  5.2944e-02 , runtime :  312 \n",
      "step :  3600 , loss : 2.7150e-05 ,  Y0 :  5.2887e-02 , runtime :  320 \n",
      "step :  3700 , loss : 2.7189e-05 ,  Y0 :  5.2915e-02 , runtime :  329 \n",
      "step :  3800 , loss : 2.6689e-05 ,  Y0 :  5.2841e-02 , runtime :  338 \n",
      "step :  3900 , loss : 2.6806e-05 ,  Y0 :  5.2854e-02 , runtime :  347 \n",
      "step :  4000 , loss : 2.6759e-05 ,  Y0 :  5.2825e-02 , runtime :  355 \n",
      " running time :  355.942 s \n",
      "4 run:\n",
      "Begin to solve Allen - Cahn equation\n",
      "End build\n",
      "step :     0 , loss : 1.5759e-01 , Y0 :  5.4381e-01 , runtime :    3 \n",
      "step :   100 , loss : 1.2433e-01 ,  Y0 :  4.9530e-01 , runtime :   12 \n",
      "step :   200 , loss : 9.7700e-02 ,  Y0 :  4.5026e-01 , runtime :   21 \n",
      "step :   300 , loss : 7.6901e-02 ,  Y0 :  4.0864e-01 , runtime :   30 \n",
      "step :   400 , loss : 5.9983e-02 ,  Y0 :  3.7021e-01 , runtime :   39 \n",
      "step :   500 , loss : 4.6576e-02 ,  Y0 :  3.3453e-01 , runtime :   47 \n",
      "step :   600 , loss : 3.5864e-02 ,  Y0 :  3.0149e-01 , runtime :   56 \n",
      "step :   700 , loss : 2.7240e-02 ,  Y0 :  2.7106e-01 , runtime :   65 \n",
      "step :   800 , loss : 2.0469e-02 ,  Y0 :  2.4297e-01 , runtime :   74 \n",
      "step :   900 , loss : 1.5299e-02 ,  Y0 :  2.1726e-01 , runtime :   82 \n",
      "step :  1000 , loss : 1.1310e-02 ,  Y0 :  1.9388e-01 , runtime :   91 \n",
      "step :  1100 , loss : 8.1702e-03 ,  Y0 :  1.7276e-01 , runtime :  100 \n",
      "step :  1200 , loss : 5.7934e-03 ,  Y0 :  1.5384e-01 , runtime :  108 \n",
      "step :  1300 , loss : 4.0367e-03 ,  Y0 :  1.3711e-01 , runtime :  116 \n",
      "step :  1400 , loss : 2.7348e-03 ,  Y0 :  1.2232e-01 , runtime :  125 \n",
      "step :  1500 , loss : 1.8226e-03 ,  Y0 :  1.0944e-01 , runtime :  134 \n",
      "step :  1600 , loss : 1.1893e-03 ,  Y0 :  9.8394e-02 , runtime :  142 \n",
      "step :  1700 , loss : 7.6077e-04 ,  Y0 :  8.9015e-02 , runtime :  151 \n",
      "step :  1800 , loss : 4.8483e-04 ,  Y0 :  8.1199e-02 , runtime :  160 \n",
      "step :  1900 , loss : 3.0158e-04 ,  Y0 :  7.4715e-02 , runtime :  169 \n",
      "step :  2000 , loss : 1.8693e-04 ,  Y0 :  6.9430e-02 , runtime :  177 \n",
      "step :  2100 , loss : 1.1936e-04 ,  Y0 :  6.5248e-02 , runtime :  186 \n",
      "step :  2200 , loss : 7.7774e-05 ,  Y0 :  6.1924e-02 , runtime :  195 \n",
      "step :  2300 , loss : 5.4394e-05 ,  Y0 :  5.9421e-02 , runtime :  204 \n",
      "step :  2400 , loss : 4.1548e-05 ,  Y0 :  5.7526e-02 , runtime :  213 \n",
      "step :  2500 , loss : 3.4618e-05 ,  Y0 :  5.6088e-02 , runtime :  222 \n",
      "step :  2600 , loss : 3.1194e-05 ,  Y0 :  5.5006e-02 , runtime :  230 \n",
      "step :  2700 , loss : 2.9255e-05 ,  Y0 :  5.4301e-02 , runtime :  239 \n",
      "step :  2800 , loss : 2.8734e-05 ,  Y0 :  5.3807e-02 , runtime :  248 \n",
      "step :  2900 , loss : 2.7776e-05 ,  Y0 :  5.3458e-02 , runtime :  257 \n",
      "step :  3000 , loss : 2.7604e-05 ,  Y0 :  5.3278e-02 , runtime :  266 \n",
      "step :  3100 , loss : 2.7342e-05 ,  Y0 :  5.3160e-02 , runtime :  275 \n",
      "step :  3200 , loss : 2.6796e-05 ,  Y0 :  5.3056e-02 , runtime :  283 \n",
      "step :  3300 , loss : 2.6701e-05 ,  Y0 :  5.3038e-02 , runtime :  292 \n",
      "step :  3400 , loss : 2.6920e-05 ,  Y0 :  5.2979e-02 , runtime :  301 \n",
      "step :  3500 , loss : 2.6505e-05 ,  Y0 :  5.2943e-02 , runtime :  310 \n",
      "step :  3600 , loss : 2.6473e-05 ,  Y0 :  5.2934e-02 , runtime :  318 \n",
      "step :  3700 , loss : 2.6249e-05 ,  Y0 :  5.2902e-02 , runtime :  327 \n",
      "step :  3800 , loss : 2.6270e-05 ,  Y0 :  5.2890e-02 , runtime :  336 \n",
      "step :  3900 , loss : 2.6093e-05 ,  Y0 :  5.2866e-02 , runtime :  345 \n",
      "step :  4000 , loss : 2.6122e-05 ,  Y0 :  5.2900e-02 , runtime :  354 \n",
      " running time :  354.449 s \n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import math\n",
    "import tensorflow.compat.v1 as tf\n",
    "import numpy as np\n",
    "from tensorflow.python.training.moving_averages import assign_moving_average  # Moving average\n",
    "from scipy.stats import multivariate_normal as normal  # Generate normally distributed random numbers\n",
    "from tensorflow.python.ops import control_flow_ops  # Used for control flow\n",
    "from tensorflow import random_normal_initializer as norm_init  # Tensor initializer with normal distribution\n",
    "from tensorflow import random_uniform_initializer as unif_init  # Tensor initializer with uniform distribution\n",
    "from tensorflow import constant_initializer as const_init  # Tensor initializer with constant values\n",
    "from tensorflow.keras.layers import LSTMCell\n",
    "\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "class SolveAllenCahn(object):\n",
    "    \"\"\" The fully-connected neural network model \"\"\"\n",
    "    def __init__(self, sess):\n",
    "        self.sess = sess  # Session\n",
    "        # parameters for the PDE\n",
    "        self.d = 100  # Dimension of the data\n",
    "        self.T = 0.3  # Time length for each path\n",
    "        # parameters for the algorithm\n",
    "        self.n_time = 20  # Number of networks\n",
    "        self.batch_size = 64  # Use paths in batches for calculations, 64*4=256\n",
    "        self.valid_size = 256  # 256 Monte Carlo samples (paths)\n",
    "        self.n_maxstep = 4000  # Iteration steps\n",
    "        self.n_displaystep = 100  # Print every 100 steps\n",
    "        self.learning_rate = 5e-4  # Learning rate\n",
    "        self.Yini = [0.3,0.6]  # Min and max values for initial Y0\n",
    "        # some basic constants and variables\n",
    "        self.h = (self.T + 0.0) / self.n_time  # Data time interval for each path, delta t\n",
    "        self.sqrth = math.sqrt(self.h)  # sqrt(delta t), used later in calculations\n",
    "        self.t_stamp = np.arange(0, self.n_time) * self.h  # Time stamps, cumulative time\n",
    "        self._extra_train_ops = []  # Batch moving average operation, includes additional trainable beta and gamma\n",
    "\n",
    "    def train(self):\n",
    "        # Main function for neural network training\n",
    "        start_time = time.time()  # Start time\n",
    "        # Create new TensorFlow variable, name='global_step', as a generator for consistency\n",
    "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(1),\n",
    "                                           trainable=False, dtype=tf.int32)  # Not added to the trainable variable list, just a counter\n",
    "        trainable_vars = tf.trainable_variables()  # View trainable variables\n",
    "        grads = tf.gradients(self.loss, trainable_vars)  # Gradients for loss trainable variables\n",
    "        optimizer = tf.train.AdamOptimizer(self.learning_rate)  # Gradient optimizer\n",
    "        apply_op = optimizer.apply_gradients(zip(grads, trainable_vars),  # Apply gradients to update trainable_vars list\n",
    "                                             global_step=self.global_step)  # Update gradients and iteration count\n",
    "\n",
    "        train_ops = [apply_op] + self._extra_train_ops  # Add operations, similar to list1.extend(list2)\n",
    "        self.train_op = tf.group(*train_ops)  # tf.group(*train_ops) combines operations in *train_ops\n",
    "\n",
    "        self.loss_history = []  # To record loss values\n",
    "        self.init_history = []  # To record Y0 values\n",
    "        self.run_time = []  # To record runtime\n",
    "\n",
    "        # For validation, 256 Monte Carlo samples as the validation set\n",
    "        dW_valid, X_valid = self.sample_path(self.valid_size)  # Generate data\n",
    "        feed_dict_valid = {self.dW: dW_valid,  # Feed data to placeholders in buildmodel\n",
    "                           self.X: X_valid,\n",
    "                           self.is_training: False}  # Exclude from iteration\n",
    "        # Initialization\n",
    "        step = 1\n",
    "        self.sess.run(tf.global_variables_initializer())  # Initialize global variables\n",
    "\n",
    "        # Run framework\n",
    "        temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)  # Calculate loss\n",
    "\n",
    "        temp_init = self.Y0.eval()[0]  # Extract values, Y0 is a 2D tensor\n",
    "        self.loss_history.append(temp_loss)  # Record loss\n",
    "        self.init_history.append(temp_init)  # Record Y0\n",
    "        self.run_time.append(time.time() - start_time + self.t_bd)\n",
    "        print(\"step : %5u , loss : %.4e , \" % \\\n",
    "              (0, temp_loss) + \\\n",
    "              \"Y0 : % .4e , runtime : %4u \" % \\\n",
    "              (temp_init, time.time() - start_time + self.t_bd))  # Print state for step=0\n",
    "\n",
    "        # Start SGD iteration, steps 0-4000\n",
    "        for _ in range(self.n_maxstep + 1):\n",
    "            step = self.sess.run(self.global_step)\n",
    "            dW_train, X_train = self.sample_path(self.batch_size)  # Generate data\n",
    "            self.sess.run(self.train_op,\n",
    "                          feed_dict={self.dW: dW_train,  # Feed data to placeholders in buildmodel\n",
    "                                     self.X: X_train,\n",
    "                                     self.is_training: True})\n",
    "            if step % self.n_displaystep == 0:  # Test with validation set every 100 steps for loss and Y0\n",
    "                temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)\n",
    "                temp_init = self.Y0.eval()[0]  # Extract values\n",
    "                self.loss_history.append(temp_loss)  # Loss values\n",
    "                self.init_history.append(temp_init)  # Y0 values, Y0 is a 2D tensor\n",
    "                self.run_time.append(time.time() - start_time + self.t_bd)\n",
    "                print(\"step : % 5u , loss : %.4e , \" % \\\n",
    "                      (step, temp_loss) + \\\n",
    "                      \" Y0 : % .4e , runtime : %4u \" % \\\n",
    "                      (temp_init, time.time() - start_time + self.t_bd))\n",
    "            step += 1\n",
    "        end_time = time.time()  # End time for training\n",
    "        print(\" running time : % .3f s \" % \\\n",
    "              (end_time - start_time + self.t_bd))\n",
    "\n",
    "    def build(self):\n",
    "        # Build the whole network by stacking subnetworks\n",
    "        start_time = time.time()\n",
    "        # Placeholders for dW, X, and is_training, batch size is None to process one batch at a time\n",
    "        self.dW = tf.placeholder(tf.float32, [None, self.d, self.n_time], name='dW')  # None*100*20\n",
    "        self.X = tf.placeholder(tf.float32, [None, self.d, self.n_time + 1], name='X')  # None*100*20\n",
    "        self.is_training = tf.placeholder(tf.bool)\n",
    "\n",
    "        # Initialize Y0 and Z0\n",
    "        self.Y0 = tf.Variable(tf.random_uniform([1],  # Initial u0, one value for one dimension\n",
    "                                                minval=self.Yini[0],  # Min value 0.3\n",
    "                                                maxval=self.Yini[1],  # Max value 0.6\n",
    "                                                dtype=tf.float32))\n",
    "        self.Z0 = tf.Variable(tf.random_uniform([1, self.d],  # Initial value for gradient u, a 1*d vector\n",
    "                                                minval=-.1,  # Min value\n",
    "                                                maxval=.1,  # Max value\n",
    "                                                dtype=tf.float32))\n",
    "        self.allones = tf.ones(shape=tf.stack([tf.shape(self.dW)[0], 1]),  # tf.shape(self.dW)[0]=len(batch), shape=(batch,1)\n",
    "                               dtype=tf.float32)  # Batch generation of initial values\n",
    "\n",
    "        Y = self.allones * self.Y0  # Initial Y as input, each batch gets the same initial Y value, Y is a (batch,1) 2D matrix [[],[],..,]\n",
    "        Z = tf.matmul(self.allones, self.Z0)  # Initial Z as output, similar to Y, but as Z is a vector, itâ€™s a (batch, d) matrix\n",
    "\n",
    "        with tf.variable_scope('forward', reuse=tf.AUTO_REUSE):  # Forward propagation\n",
    "            cell = LSTMCell(units=110)\n",
    "            batch_size = tf.shape(self.X)[0]\n",
    "            hid = [tf.zeros([batch_size, cell.units]), tf.zeros([batch_size, cell.units])]\n",
    "            w = tf.get_variable('Matrixhid',\n",
    "                                [110, 100], tf.float32,\n",
    "                                norm_init(stddev=5 / np.sqrt(110)))\n",
    "            for t in range(0, self.n_time - 1):  # Network for the first N-1 xt\n",
    "                # Computation from recursive formula\n",
    "                Y = Y - self.f_tf(self.t_stamp[t],  # Timestamp, cumulative time value\n",
    "                                  self.X[:, :, t], Y, Z) * self.h  # Y.shape=(batch,1)\n",
    "                Y = Y + tf.reduce_sum(Z * self.dW[:, :, t], 1, keep_dims=True)  # Intermediate time output.\n",
    "                output, hid = cell(self._batch_norm(self.X[:, :, t + 1], 'normal'), hid)  # Update hid at each time\n",
    "                Z = tf.nn.relu(tf.matmul(output, w)) / self.d\n",
    "\n",
    "            # Terminal time, as final Y does not use neural network\n",
    "            Y = Y - self.f_tf(self.t_stamp[self.n_time - 1],  # -1 because index starts from 0, terminal step is different\n",
    "                              self.X[:, :, self.n_time - 1], Y, Z) * self.h\n",
    "            Y = Y + tf.reduce_sum(Z * self.dW[:, :, self.n_time - 1], 1, keep_dims=True)  # Final Y output\n",
    "            term_delta = Y - self.g_tf(self.T, self.X[:, :, self.n_time])  # Loss function\n",
    "            self.clipped_delta = tf.clip_by_value(term_delta, -50.0, 50.0)  # Clip values within bounds\n",
    "            self.loss = tf.reduce_mean(self.clipped_delta ** 2)  # Calculate loss\n",
    "        self.t_bd = time.time() - start_time  # Time to generate the network\n",
    "\n",
    "    def sample_path(self, n_sample):\n",
    "        # Generate paths, creating (xt, (wt-wt-1))\n",
    "        dW_sample = np.zeros([n_sample, self.d, self.n_time])  # Sample count, dimension, time length\n",
    "        X_sample = np.zeros([n_sample, self.d, self.n_time + 1])\n",
    "        for i in range(self.n_time):  # Generate one column at a time\n",
    "            dW_sample[:, :, i] = np.reshape(normal.rvs(mean=np.zeros(self.d),  # This function is similar to np.random.normal()\n",
    "                                                       cov=1,  # Why not std=1 ?\n",
    "                                                       size=n_sample) * self.sqrth,  # sqrt(delta t), W(t)-W(s) is independent, with mean 0 and variance t-s\n",
    "                                              (n_sample, self.d))\n",
    "            X_sample[:, :, i + 1] = X_sample[:, :, i] + np.sqrt(2) * dW_sample[:, :, i]  # From formula\n",
    "        return dW_sample, X_sample\n",
    "\n",
    "    def f_tf(self, t, X, Y, Z):\n",
    "        # Nonlinear term\n",
    "        return Y - tf.pow(Y, 3)\n",
    "\n",
    "    def g_tf(self, t, X):\n",
    "        # Terminal conditions\n",
    "        return 0.5 / (1 + 0.2 * tf.reduce_sum(X ** 2, 1, keep_dims=True))\n",
    "\n",
    "    def _batch_norm(self, x, name):\n",
    "        \"\"\" Batch normalization \"\"\"  # Beta and gamma are trainable, third type of parameter, needs 2 columns for each normalization\n",
    "        with tf.variable_scope(name):\n",
    "            params_shape = [x.get_shape()[-1]]  # [d, d+10, d+10, d], first dimension is batch\n",
    "            beta = tf.get_variable('beta', params_shape, tf.float32, norm_init(0.0, stddev=0.1))\n",
    "            gamma = tf.get_variable('gamma', params_shape, tf.float32, unif_init(0.1, 0.5))\n",
    "            mv_mean = tf.get_variable('moving_mean',  # Moving_mean improves mean for different batches\n",
    "                                      params_shape, tf.float32, const_init(0.0), trainable=False)\n",
    "            mv_var = tf.get_variable('moving_variance',\n",
    "                                     params_shape, tf.float32, const_init(1.0), trainable=False)\n",
    "\n",
    "            # These ops will only be performed when training\n",
    "            mean, variance = tf.nn.moments(x, [0], name='moments')  # Centered dimension for normalization, [0] means batch, calculate mean and variance for 64 samples\n",
    "            self._extra_train_ops.append(assign_moving_average(mv_mean, mean, 0.99))  # Explained below\n",
    "            self._extra_train_ops.append(assign_moving_average(mv_var, variance, 0.99))\n",
    "\n",
    "            mean, variance = tf.cond(self.is_training,  # control_flow_ops.cond controls execution flow, first parameter is the condition\n",
    "                                                   lambda: (mean, variance),  # If True, calculate mean and variance\n",
    "                                                   lambda: (mv_mean, mv_var))  # If False, directly use smoothed value\n",
    "\n",
    "            y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
    "            # Above operation is equivalent to:\n",
    "            # y = (y - mean) / tf.sqrt(variance+1e-6)  # 1e-6 epsilon\n",
    "            # y = y * gamma + beta\n",
    "\n",
    "            # Ensure that shape is maintained after normalization\n",
    "            y.set_shape(x.get_shape())\n",
    "            return y\n",
    "\n",
    "\n",
    "def main(name):\n",
    "    tf.reset_default_graph()\n",
    "    with tf.Session() as sess:\n",
    "        tf.set_random_seed(1)  # Random seed in tf\n",
    "        print(\"Begin to solve Allen - Cahn equation\")\n",
    "        model = SolveAllenCahn(sess)  # Create object\n",
    "        model.build()  # Call object method, constructs a model but does not feed data\n",
    "        print('End build')\n",
    "        model.train()  # Generate and feed data into build\n",
    "        output = np.zeros((len(model.init_history), 4))  # Initialize result as 0, fill later\n",
    "        output[:, 0] = np.arange(len(model.init_history)) * model.n_displaystep  # Output step\n",
    "        output[:, 1] = model.loss_history  # Output loss list\n",
    "        output[:, 2] = model.init_history  # Output Y0 list\n",
    "        output[:, 3] = model.run_time\n",
    "        np.savetxt(\"./AllenCahn_d100_RNN\" + str(name) + \".csv\",  # Save output results\n",
    "                   output,\n",
    "                   fmt=['%d', '%.5e', '%.5e', '%d'],\n",
    "                   delimiter=\",\",\n",
    "                   header=\"step ,loss function , target value , runtime \",\n",
    "                   comments='')\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    np.random.seed(1)  # Define a random seed\n",
    "    for i in range(5):\n",
    "        print(str(i) + ' run:')\n",
    "        main(i)  # Run main program\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bCP1zuuBpndl",
    "outputId": "f4c6e301-0d9b-4ba6-ba9d-15aa42fdf007"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
      "    steps  loss_function1  loss_function2  loss_function3  loss_function4  \\\n",
      "0       0        0.162569        0.158931        0.163812        0.165382   \n",
      "1     100        0.127197        0.124704        0.128258        0.130386   \n",
      "2     200        0.099606        0.097599        0.100753        0.102302   \n",
      "3     300        0.078282        0.076727        0.079387        0.080293   \n",
      "4     400        0.060937        0.059759        0.061818        0.063093   \n",
      "5     500        0.047326        0.046334        0.047630        0.049134   \n",
      "6     600        0.036222        0.035546        0.036430        0.038023   \n",
      "7     700        0.027688        0.027199        0.027867        0.028913   \n",
      "8     800        0.020818        0.020504        0.020898        0.021855   \n",
      "9     900        0.015454        0.015314        0.015575        0.016235   \n",
      "10   1000        0.011271        0.011311        0.011390        0.011962   \n",
      "11   1100        0.008135        0.008123        0.008222        0.008623   \n",
      "12   1200        0.005758        0.005749        0.005840        0.006127   \n",
      "13   1300        0.004016        0.004005        0.004033        0.004279   \n",
      "14   1400        0.002733        0.002733        0.002727        0.002949   \n",
      "15   1500        0.001839        0.001828        0.001827        0.002000   \n",
      "16   1600        0.001199        0.001196        0.001191        0.001315   \n",
      "17   1700        0.000765        0.000760        0.000753        0.000852   \n",
      "18   1800        0.000483        0.000483        0.000467        0.000536   \n",
      "19   1900        0.000300        0.000300        0.000292        0.000337   \n",
      "20   2000        0.000184        0.000183        0.000181        0.000209   \n",
      "21   2100        0.000117        0.000114        0.000114        0.000132   \n",
      "22   2200        0.000076        0.000074        0.000076        0.000087   \n",
      "23   2300        0.000053        0.000053        0.000054        0.000060   \n",
      "24   2400        0.000040        0.000041        0.000043        0.000046   \n",
      "25   2500        0.000034        0.000035        0.000037        0.000037   \n",
      "26   2600        0.000030        0.000032        0.000034        0.000033   \n",
      "27   2700        0.000029        0.000031        0.000032        0.000031   \n",
      "28   2800        0.000028        0.000030        0.000032        0.000029   \n",
      "29   2900        0.000028        0.000030        0.000031        0.000029   \n",
      "30   3000        0.000028        0.000029        0.000030        0.000028   \n",
      "31   3100        0.000027        0.000030        0.000030        0.000027   \n",
      "32   3200        0.000027        0.000030        0.000030        0.000027   \n",
      "33   3300        0.000027        0.000029        0.000030        0.000027   \n",
      "34   3400        0.000027        0.000030        0.000030        0.000027   \n",
      "35   3500        0.000028        0.000030        0.000031        0.000027   \n",
      "36   3600        0.000028        0.000030        0.000030        0.000027   \n",
      "37   3700        0.000027        0.000030        0.000030        0.000027   \n",
      "38   3800        0.000028        0.000030        0.000030        0.000027   \n",
      "39   3900        0.000027        0.000030        0.000030        0.000027   \n",
      "40   4000        0.000027        0.000030        0.000029        0.000027   \n",
      "\n",
      "    loss_function5  Mean_loss  Std_loss  target_value1  target_value2  ...  \\\n",
      "0         0.157589   0.161657  0.002944       0.543809       0.543809  ...   \n",
      "1         0.124330   0.126975  0.002258       0.495207       0.495327  ...   \n",
      "2         0.097700   0.099592  0.001802       0.450111       0.450445  ...   \n",
      "3         0.076901   0.078318  0.001385       0.408473       0.408838  ...   \n",
      "4         0.059983   0.061118  0.001229       0.369926       0.370181  ...   \n",
      "5         0.046575   0.047400  0.000988       0.334127       0.334352  ...   \n",
      "6         0.035863   0.036417  0.000858       0.301129       0.301244  ...   \n",
      "7         0.027240   0.027781  0.000621       0.270572       0.270762  ...   \n",
      "8         0.020470   0.020909  0.000502       0.242439       0.242694  ...   \n",
      "9         0.015299   0.015575  0.000345       0.216691       0.217048  ...   \n",
      "10        0.011310   0.011449  0.000260       0.193343       0.193796  ...   \n",
      "11        0.008170   0.008254  0.000188       0.172261       0.172672  ...   \n",
      "12        0.005793   0.005853  0.000140       0.153515       0.153845  ...   \n",
      "13        0.004037   0.004074  0.000103       0.136776       0.137094  ...   \n",
      "14        0.002735   0.002775  0.000087       0.122031       0.122348  ...   \n",
      "15        0.001823   0.001863  0.000069       0.109228       0.109519  ...   \n",
      "16        0.001189   0.001218  0.000049       0.098166       0.098435  ...   \n",
      "17        0.000761   0.000778  0.000037       0.088806       0.089021  ...   \n",
      "18        0.000485   0.000491  0.000024       0.080997       0.081210  ...   \n",
      "19        0.000302   0.000306  0.000016       0.074594       0.074756  ...   \n",
      "20        0.000187   0.000189  0.000010       0.069412       0.069514  ...   \n",
      "21        0.000119   0.000119  0.000007       0.065302       0.065294  ...   \n",
      "22        0.000078   0.000078  0.000004       0.062005       0.061975  ...   \n",
      "23        0.000054   0.000055  0.000002       0.059485       0.059445  ...   \n",
      "24        0.000042   0.000042  0.000002       0.057561       0.057496  ...   \n",
      "25        0.000035   0.000035  0.000001       0.056139       0.056067  ...   \n",
      "26        0.000031   0.000032  0.000001       0.055104       0.055043  ...   \n",
      "27        0.000029   0.000030  0.000001       0.054393       0.054304  ...   \n",
      "28        0.000029   0.000030  0.000001       0.053884       0.053807  ...   \n",
      "29        0.000028   0.000029  0.000001       0.053533       0.053494  ...   \n",
      "30        0.000028   0.000029  0.000001       0.053269       0.053265  ...   \n",
      "31        0.000027   0.000028  0.000001       0.053161       0.053066  ...   \n",
      "32        0.000027   0.000028  0.000002       0.053066       0.052965  ...   \n",
      "33        0.000027   0.000028  0.000001       0.052983       0.052940  ...   \n",
      "34        0.000027   0.000028  0.000001       0.052909       0.052923  ...   \n",
      "35        0.000027   0.000028  0.000002       0.052919       0.052868  ...   \n",
      "36        0.000026   0.000028  0.000002       0.052884       0.052901  ...   \n",
      "37        0.000026   0.000028  0.000002       0.052905       0.052903  ...   \n",
      "38        0.000026   0.000028  0.000002       0.052851       0.052808  ...   \n",
      "39        0.000026   0.000028  0.000002       0.052876       0.052806  ...   \n",
      "40        0.000026   0.000028  0.000002       0.052907       0.052918  ...   \n",
      "\n",
      "    target_value5   Mean_Y0    Std_Y0  runtime1  runtime2  runtime3  runtime4  \\\n",
      "0        0.543809  0.543809  0.000000         3         3         2         2   \n",
      "1        0.495297  0.495243  0.000057        13        13        12        12   \n",
      "2        0.450263  0.450292  0.000109        22        20        21        21   \n",
      "3        0.408636  0.408703  0.000133        31        29        30        30   \n",
      "4        0.370208  0.370118  0.000108        40        38        39        39   \n",
      "5        0.334528  0.334331  0.000132        48        46        48        47   \n",
      "6        0.301488  0.301263  0.000146        57        55        57        56   \n",
      "7        0.271059  0.270751  0.000178        65        64        65        65   \n",
      "8        0.242966  0.242665  0.000190        74        72        74        74   \n",
      "9        0.217263  0.216980  0.000197        82        80        83        83   \n",
      "10       0.193881  0.193635  0.000203        90        89        92        92   \n",
      "11       0.172761  0.172504  0.000207        99        98       101       101   \n",
      "12       0.153836  0.153655  0.000172       107       106       110       109   \n",
      "13       0.137106  0.136912  0.000167       116       115       119       118   \n",
      "14       0.122324  0.122171  0.000149       125       123       128       127   \n",
      "15       0.109439  0.109367  0.000112       133       132       136       136   \n",
      "16       0.098394  0.098321  0.000098       142       141       145       145   \n",
      "17       0.089015  0.088954  0.000079       151       149       154       153   \n",
      "18       0.081199  0.081118  0.000080       159       158       163       162   \n",
      "19       0.074715  0.074671  0.000057       168       167       172       171   \n",
      "20       0.069430  0.069447  0.000035       177       176       181       180   \n",
      "21       0.065248  0.065277  0.000022       186       185       190       188   \n",
      "22       0.061924  0.061977  0.000030       194       193       198       197   \n",
      "23       0.059421  0.059451  0.000021       203       202       207       206   \n",
      "24       0.057526  0.057535  0.000025       212       211       216       215   \n",
      "25       0.056088  0.056119  0.000037       221       220       225       224   \n",
      "26       0.055006  0.055078  0.000048       230       229       234       232   \n",
      "27       0.054301  0.054368  0.000055       238       238       242       241   \n",
      "28       0.053807  0.053859  0.000045       247       246       251       250   \n",
      "29       0.053458  0.053513  0.000038       255       255       260       259   \n",
      "30       0.053278  0.053271  0.000022       264       264       269       267   \n",
      "31       0.053160  0.053122  0.000040       273       273       277       276   \n",
      "32       0.053056  0.053048  0.000042       282       282       286       285   \n",
      "33       0.053038  0.052997  0.000038       290       290       294       294   \n",
      "34       0.052979  0.052947  0.000034       299       299       302       303   \n",
      "35       0.052943  0.052920  0.000028       307       308       311       312   \n",
      "36       0.052935  0.052899  0.000019       316       317       319       320   \n",
      "37       0.052902  0.052904  0.000006       324       326       328       329   \n",
      "38       0.052890  0.052859  0.000035       333       335       337       338   \n",
      "39       0.052866  0.052852  0.000024       341       343       345       347   \n",
      "40       0.052900  0.052888  0.000033       350       352       353       355   \n",
      "\n",
      "    runtime5  Mean_time  Std_time  \n",
      "0          3        2.6  0.489898  \n",
      "1         12       12.4  0.489898  \n",
      "2         21       21.0  0.632456  \n",
      "3         30       30.0  0.632456  \n",
      "4         39       39.0  0.632456  \n",
      "5         47       47.2  0.748331  \n",
      "6         56       56.2  0.748331  \n",
      "7         65       64.8  0.400000  \n",
      "8         74       73.6  0.800000  \n",
      "9         82       82.0  1.095445  \n",
      "10        91       90.8  1.166190  \n",
      "11       100       99.8  1.166190  \n",
      "12       108      108.0  1.414214  \n",
      "13       116      116.8  1.469694  \n",
      "14       125      125.6  1.743560  \n",
      "15       134      134.2  1.600000  \n",
      "16       142      143.0  1.673320  \n",
      "17       151      151.6  1.743560  \n",
      "18       160      160.4  1.854724  \n",
      "19       169      169.4  1.854724  \n",
      "20       177      178.2  1.939072  \n",
      "21       186      187.0  1.788854  \n",
      "22       195      195.4  1.854724  \n",
      "23       204      204.4  1.854724  \n",
      "24       213      213.4  1.854724  \n",
      "25       222      222.4  1.854724  \n",
      "26       230      231.0  1.788854  \n",
      "27       239      239.6  1.624808  \n",
      "28       248      248.4  1.854724  \n",
      "29       257      257.2  2.039608  \n",
      "30       266      266.0  1.897367  \n",
      "31       275      274.8  1.600000  \n",
      "32       283      283.6  1.624808  \n",
      "33       292      292.0  1.788854  \n",
      "34       301      300.8  1.600000  \n",
      "35       310      309.6  1.854724  \n",
      "36       318      318.0  1.414214  \n",
      "37       327      326.8  1.720465  \n",
      "38       336      335.8  1.720465  \n",
      "39       345      344.2  2.039608  \n",
      "40       354      352.8  1.720465  \n",
      "\n",
      "[41 rows x 22 columns]\n"
     ]
    }
   ],
   "source": [
    "!pip install openpyxl\n",
    "import pandas as pd\n",
    "import openpyxl\n",
    "from openpyxl.styles import PatternFill, Font\n",
    "\n",
    "# Load the data\n",
    "dataframes = {}\n",
    "for i in range(5):\n",
    "    df = pd.read_csv(f'AllenCahn_d100_RNN{i}.csv')\n",
    "    df.columns = ['steps', f'loss_function{i+1}', f'target_value{i+1}', f'runtime{i+1}']\n",
    "    if i == 0:\n",
    "        merged_df = df\n",
    "    else:\n",
    "        merged_df = pd.merge(merged_df, df, on='steps', how='outer')\n",
    "\n",
    "# Calculate mean and standard deviation, ensuring precision\n",
    "merged_df['Mean_loss'] = merged_df[[f'loss_function{i+1}' for i in range(5)]].mean(axis=1).round(8)\n",
    "merged_df['Std_loss'] = merged_df[[f'loss_function{i+1}' for i in range(5)]].std(axis=1, ddof=0)  # ddof=0 for population standard deviation\n",
    "merged_df['Mean_Y0'] = merged_df[[f'target_value{i+1}' for i in range(5)]].mean(axis=1).round(7)\n",
    "merged_df['Std_Y0'] = merged_df[[f'target_value{i+1}' for i in range(5)]].std(axis=1, ddof=0)\n",
    "merged_df['Mean_time'] = merged_df[[f'runtime{i+1}' for i in range(5)]].mean(axis=1).round(1)\n",
    "merged_df['Std_time'] = merged_df[[f'runtime{i+1}' for i in range(5)]].std(axis=1, ddof=0)\n",
    "\n",
    "\n",
    "# Adjust the column order to ensure it matches the specified layout\n",
    "columns_order = [\n",
    "    'steps',\n",
    "    'loss_function1', 'loss_function2', 'loss_function3', 'loss_function4', 'loss_function5',\n",
    "    'Mean_loss', 'Std_loss',\n",
    "    'target_value1', 'target_value2', 'target_value3', 'target_value4', 'target_value5',\n",
    "    'Mean_Y0', 'Std_Y0',\n",
    "    'runtime1', 'runtime2', 'runtime3', 'runtime4', 'runtime5',\n",
    "    'Mean_time', 'Std_time'\n",
    "]\n",
    "merged_df = merged_df[columns_order]\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "merged_df.to_excel('drnn_data.xlsx', index=False)\n",
    "\n",
    "# Load the Excel file\n",
    "wb = openpyxl.load_workbook('drnn_data.xlsx')\n",
    "ws = wb.active\n",
    "\n",
    "# Set the highlight format\n",
    "yellow_fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
    "bold_font = Font(bold=True)  # Define bold font\n",
    "\n",
    "# Highlight specific rows and bold specific columns\n",
    "highlight_steps = [0, 1000, 2000, 3000, 4000]\n",
    "bold_columns = ['Mean_loss', 'Std_loss', 'Mean_Y0', 'Std_Y0', 'Mean_time', 'Std_time']\n",
    "bold_columns_indices = []\n",
    "\n",
    "# Determine the column indices for bolding\n",
    "for col in ws[1]:  # Assuming the first row contains header names\n",
    "    if col.value in bold_columns:\n",
    "        bold_columns_indices.append(col.column)\n",
    "\n",
    "for row in ws.iter_rows():\n",
    "    for cell in row:\n",
    "        # Apply bold formatting to specified columns\n",
    "        if cell.column in bold_columns_indices:\n",
    "            cell.font = bold_font\n",
    "        # Set the background color to yellow for specific rows\n",
    "        if row[0].value in highlight_steps:\n",
    "            cell.fill = yellow_fill\n",
    "\n",
    "# Save the formatted Excel file\n",
    "wb.save('drnn_data.xlsx')\n",
    "\n",
    "# Load the Excel file to display the data\n",
    "data = pd.read_excel('drnn_data.xlsx')\n",
    "print(data)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "TPU",
  "colab": {
   "gpuType": "V28",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

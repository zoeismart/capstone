{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkEQbs8AJwWH"
      },
      "source": [
        "##### Deep Learning to Solve Higher Differential Equations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uJeCw7kaJwWJ",
        "outputId": "cc31285b-aa5f-40b4-8f29-f7075220ac2b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n",
            "2.17.0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "     raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))\n",
        "print(tf.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIDPN9CBJwWP",
        "outputId": "6544211d-bc28-40b3-915f-73cee06c83f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0 run:\n",
            "Begin to solve Allen - Cahn equation\n",
            "End build\n",
            "step :     0 , loss : 1.6388e-01 , Y0 :  5.4381e-01 , runtime :    7 \n",
            "step :   100 , loss : 1.2872e-01 ,  Y0 :  4.9521e-01 , runtime :   31 \n",
            "step :   200 , loss : 1.0083e-01 ,  Y0 :  4.5013e-01 , runtime :   51 \n",
            "step :   300 , loss : 7.9346e-02 ,  Y0 :  4.0851e-01 , runtime :   73 \n",
            "step :   400 , loss : 6.2092e-02 ,  Y0 :  3.6999e-01 , runtime :   92 \n",
            "step :   500 , loss : 4.8334e-02 ,  Y0 :  3.3424e-01 , runtime :  114 \n",
            "step :   600 , loss : 3.6952e-02 ,  Y0 :  3.0123e-01 , runtime :  134 \n",
            "step :   700 , loss : 2.8076e-02 ,  Y0 :  2.7067e-01 , runtime :  156 \n",
            "step :   800 , loss : 2.0854e-02 ,  Y0 :  2.4251e-01 , runtime :  178 \n",
            "step :   900 , loss : 1.5507e-02 ,  Y0 :  2.1678e-01 , runtime :  197 \n",
            "step :  1000 , loss : 1.1445e-02 ,  Y0 :  1.9347e-01 , runtime :  216 \n",
            "step :  1100 , loss : 8.2324e-03 ,  Y0 :  1.7236e-01 , runtime :  241 \n",
            "step :  1200 , loss : 5.8491e-03 ,  Y0 :  1.5355e-01 , runtime :  264 \n",
            "step :  1300 , loss : 4.0924e-03 ,  Y0 :  1.3682e-01 , runtime :  287 \n",
            "step :  1400 , loss : 2.7940e-03 ,  Y0 :  1.2212e-01 , runtime :  310 \n",
            "step :  1500 , loss : 1.8851e-03 ,  Y0 :  1.0930e-01 , runtime :  333 \n",
            "step :  1600 , loss : 1.2314e-03 ,  Y0 :  9.8254e-02 , runtime :  354 \n",
            "step :  1700 , loss : 7.9136e-04 ,  Y0 :  8.8878e-02 , runtime :  380 \n",
            "step :  1800 , loss : 5.0039e-04 ,  Y0 :  8.1035e-02 , runtime :  400 \n",
            "step :  1900 , loss : 3.1214e-04 ,  Y0 :  7.4621e-02 , runtime :  420 \n",
            "step :  2000 , loss : 1.9463e-04 ,  Y0 :  6.9389e-02 , runtime :  440 \n",
            "step :  2100 , loss : 1.2655e-04 ,  Y0 :  6.5284e-02 , runtime :  462 \n",
            "step :  2200 , loss : 8.4123e-05 ,  Y0 :  6.2000e-02 , runtime :  481 \n",
            "step :  2300 , loss : 6.0219e-05 ,  Y0 :  5.9470e-02 , runtime :  502 \n",
            "step :  2400 , loss : 4.6242e-05 ,  Y0 :  5.7554e-02 , runtime :  523 \n",
            "step :  2500 , loss : 3.8981e-05 ,  Y0 :  5.6138e-02 , runtime :  544 \n",
            "step :  2600 , loss : 3.5006e-05 ,  Y0 :  5.5110e-02 , runtime :  563 \n",
            "step :  2700 , loss : 3.2839e-05 ,  Y0 :  5.4367e-02 , runtime :  582 \n",
            "step :  2800 , loss : 3.2037e-05 ,  Y0 :  5.3868e-02 , runtime :  601 \n",
            "step :  2900 , loss : 3.1171e-05 ,  Y0 :  5.3514e-02 , runtime :  619 \n",
            "step :  3000 , loss : 3.0556e-05 ,  Y0 :  5.3283e-02 , runtime :  637 \n",
            "step :  3100 , loss : 3.0044e-05 ,  Y0 :  5.3182e-02 , runtime :  659 \n",
            "step :  3200 , loss : 2.9587e-05 ,  Y0 :  5.3067e-02 , runtime :  680 \n",
            "step :  3300 , loss : 2.9694e-05 ,  Y0 :  5.2987e-02 , runtime :  697 \n",
            "step :  3400 , loss : 2.9362e-05 ,  Y0 :  5.2911e-02 , runtime :  715 \n",
            "step :  3500 , loss : 2.9687e-05 ,  Y0 :  5.2904e-02 , runtime :  736 \n",
            "step :  3600 , loss : 2.9269e-05 ,  Y0 :  5.2868e-02 , runtime :  754 \n",
            "step :  3700 , loss : 2.8756e-05 ,  Y0 :  5.2886e-02 , runtime :  775 \n",
            "step :  3800 , loss : 2.9039e-05 ,  Y0 :  5.2854e-02 , runtime :  796 \n",
            "step :  3900 , loss : 2.8496e-05 ,  Y0 :  5.2857e-02 , runtime :  817 \n",
            "step :  4000 , loss : 2.8283e-05 ,  Y0 :  5.2891e-02 , runtime :  836 \n",
            " running time :  836.150 s \n",
            "1 run:\n",
            "Begin to solve Allen - Cahn equation\n",
            "End build\n",
            "step :     0 , loss : 1.5870e-01 , Y0 :  5.4381e-01 , runtime :    6 \n",
            "step :   100 , loss : 1.2408e-01 ,  Y0 :  4.9532e-01 , runtime :   29 \n",
            "step :   200 , loss : 9.7450e-02 ,  Y0 :  4.5044e-01 , runtime :   49 \n",
            "step :   300 , loss : 7.6433e-02 ,  Y0 :  4.0874e-01 , runtime :   69 \n",
            "step :   400 , loss : 6.0365e-02 ,  Y0 :  3.7008e-01 , runtime :   92 \n",
            "step :   500 , loss : 4.6690e-02 ,  Y0 :  3.3429e-01 , runtime :  111 \n",
            "step :   600 , loss : 3.5874e-02 ,  Y0 :  3.0119e-01 , runtime :  130 \n",
            "step :   700 , loss : 2.7305e-02 ,  Y0 :  2.7070e-01 , runtime :  150 \n",
            "step :   800 , loss : 2.0550e-02 ,  Y0 :  2.4260e-01 , runtime :  167 \n",
            "step :   900 , loss : 1.5390e-02 ,  Y0 :  2.1697e-01 , runtime :  188 \n",
            "step :  1000 , loss : 1.1239e-02 ,  Y0 :  1.9369e-01 , runtime :  206 \n",
            "step :  1100 , loss : 8.0883e-03 ,  Y0 :  1.7257e-01 , runtime :  227 \n",
            "step :  1200 , loss : 5.6998e-03 ,  Y0 :  1.5368e-01 , runtime :  245 \n",
            "step :  1300 , loss : 3.9651e-03 ,  Y0 :  1.3695e-01 , runtime :  264 \n",
            "step :  1400 , loss : 2.6853e-03 ,  Y0 :  1.2221e-01 , runtime :  284 \n",
            "step :  1500 , loss : 1.7956e-03 ,  Y0 :  1.0942e-01 , runtime :  305 \n",
            "step :  1600 , loss : 1.1738e-03 ,  Y0 :  9.8353e-02 , runtime :  323 \n",
            "step :  1700 , loss : 7.4572e-04 ,  Y0 :  8.8983e-02 , runtime :  344 \n",
            "step :  1800 , loss : 4.6914e-04 ,  Y0 :  8.1168e-02 , runtime :  366 \n",
            "step :  1900 , loss : 2.9124e-04 ,  Y0 :  7.4710e-02 , runtime :  387 \n",
            "step :  2000 , loss : 1.7906e-04 ,  Y0 :  6.9462e-02 , runtime :  407 \n",
            "step :  2100 , loss : 1.1078e-04 ,  Y0 :  6.5238e-02 , runtime :  427 \n",
            "step :  2200 , loss : 7.2699e-05 ,  Y0 :  6.1950e-02 , runtime :  446 \n",
            "step :  2300 , loss : 5.1601e-05 ,  Y0 :  5.9452e-02 , runtime :  466 \n",
            "step :  2400 , loss : 4.0233e-05 ,  Y0 :  5.7506e-02 , runtime :  485 \n",
            "step :  2500 , loss : 3.4834e-05 ,  Y0 :  5.6081e-02 , runtime :  508 \n",
            "step :  2600 , loss : 3.1967e-05 ,  Y0 :  5.5069e-02 , runtime :  529 \n",
            "step :  2700 , loss : 3.0779e-05 ,  Y0 :  5.4312e-02 , runtime :  550 \n",
            "step :  2800 , loss : 3.0399e-05 ,  Y0 :  5.3809e-02 , runtime :  569 \n",
            "step :  2900 , loss : 2.9970e-05 ,  Y0 :  5.3504e-02 , runtime :  589 \n",
            "step :  3000 , loss : 2.9559e-05 ,  Y0 :  5.3272e-02 , runtime :  608 \n",
            "step :  3100 , loss : 2.9846e-05 ,  Y0 :  5.3082e-02 , runtime :  626 \n",
            "step :  3200 , loss : 2.9818e-05 ,  Y0 :  5.3022e-02 , runtime :  648 \n",
            "step :  3300 , loss : 2.9495e-05 ,  Y0 :  5.2969e-02 , runtime :  669 \n",
            "step :  3400 , loss : 3.0070e-05 ,  Y0 :  5.2945e-02 , runtime :  689 \n",
            "step :  3500 , loss : 2.9928e-05 ,  Y0 :  5.2882e-02 , runtime :  708 \n",
            "step :  3600 , loss : 3.0288e-05 ,  Y0 :  5.2899e-02 , runtime :  729 \n",
            "step :  3700 , loss : 3.0026e-05 ,  Y0 :  5.2915e-02 , runtime :  748 \n",
            "step :  3800 , loss : 3.0149e-05 ,  Y0 :  5.2821e-02 , runtime :  767 \n",
            "step :  3900 , loss : 3.0284e-05 ,  Y0 :  5.2782e-02 , runtime :  789 \n",
            "step :  4000 , loss : 3.0263e-05 ,  Y0 :  5.2914e-02 , runtime :  812 \n",
            " running time :  812.553 s \n",
            "2 run:\n",
            "Begin to solve Allen - Cahn equation\n",
            "End build\n",
            "step :     0 , loss : 1.6356e-01 , Y0 :  5.4381e-01 , runtime :    5 \n",
            "step :   100 , loss : 1.2797e-01 ,  Y0 :  4.9523e-01 , runtime :   28 \n",
            "step :   200 , loss : 1.0046e-01 ,  Y0 :  4.5037e-01 , runtime :   48 \n",
            "step :   300 , loss : 7.9337e-02 ,  Y0 :  4.0886e-01 , runtime :   67 \n",
            "step :   400 , loss : 6.1987e-02 ,  Y0 :  3.7016e-01 , runtime :   87 \n",
            "step :   500 , loss : 4.7869e-02 ,  Y0 :  3.3436e-01 , runtime :  110 \n",
            "step :   600 , loss : 3.6746e-02 ,  Y0 :  3.0116e-01 , runtime :  129 \n",
            "step :   700 , loss : 2.8119e-02 ,  Y0 :  2.7071e-01 , runtime :  151 \n",
            "step :   800 , loss : 2.1114e-02 ,  Y0 :  2.4268e-01 , runtime :  172 \n",
            "step :   900 , loss : 1.5624e-02 ,  Y0 :  2.1700e-01 , runtime :  194 \n",
            "step :  1000 , loss : 1.1372e-02 ,  Y0 :  1.9360e-01 , runtime :  212 \n",
            "step :  1100 , loss : 8.2040e-03 ,  Y0 :  1.7241e-01 , runtime :  233 \n",
            "step :  1200 , loss : 5.8005e-03 ,  Y0 :  1.5347e-01 , runtime :  254 \n",
            "step :  1300 , loss : 4.0042e-03 ,  Y0 :  1.3674e-01 , runtime :  272 \n",
            "step :  1400 , loss : 2.7189e-03 ,  Y0 :  1.2202e-01 , runtime :  292 \n",
            "step :  1500 , loss : 1.7905e-03 ,  Y0 :  1.0922e-01 , runtime :  311 \n",
            "step :  1600 , loss : 1.1553e-03 ,  Y0 :  9.8212e-02 , runtime :  332 \n",
            "step :  1700 , loss : 7.3369e-04 ,  Y0 :  8.8902e-02 , runtime :  350 \n",
            "step :  1800 , loss : 4.6150e-04 ,  Y0 :  8.1057e-02 , runtime :  370 \n",
            "step :  1900 , loss : 2.8779e-04 ,  Y0 :  7.4601e-02 , runtime :  390 \n",
            "step :  2000 , loss : 1.7742e-04 ,  Y0 :  6.9394e-02 , runtime :  410 \n",
            "step :  2100 , loss : 1.1195e-04 ,  Y0 :  6.5237e-02 , runtime :  429 \n",
            "step :  2200 , loss : 7.3831e-05 ,  Y0 :  6.1961e-02 , runtime :  449 \n",
            "step :  2300 , loss : 5.1793e-05 ,  Y0 :  5.9421e-02 , runtime :  468 \n",
            "step :  2400 , loss : 4.0505e-05 ,  Y0 :  5.7504e-02 , runtime :  489 \n",
            "step :  2500 , loss : 3.4671e-05 ,  Y0 :  5.6136e-02 , runtime :  511 \n",
            "step :  2600 , loss : 3.1835e-05 ,  Y0 :  5.5087e-02 , runtime :  534 \n",
            "step :  2700 , loss : 3.0426e-05 ,  Y0 :  5.4420e-02 , runtime :  555 \n",
            "step :  2800 , loss : 2.9850e-05 ,  Y0 :  5.3903e-02 , runtime :  572 \n",
            "step :  2900 , loss : 2.9508e-05 ,  Y0 :  5.3505e-02 , runtime :  595 \n",
            "step :  3000 , loss : 2.8919e-05 ,  Y0 :  5.3323e-02 , runtime :  615 \n",
            "step :  3100 , loss : 2.9072e-05 ,  Y0 :  5.3158e-02 , runtime :  636 \n",
            "step :  3200 , loss : 2.8962e-05 ,  Y0 :  5.3063e-02 , runtime :  657 \n",
            "step :  3300 , loss : 2.8860e-05 ,  Y0 :  5.2988e-02 , runtime :  678 \n",
            "step :  3400 , loss : 2.9097e-05 ,  Y0 :  5.2926e-02 , runtime :  696 \n",
            "step :  3500 , loss : 2.9317e-05 ,  Y0 :  5.2938e-02 , runtime :  717 \n",
            "step :  3600 , loss : 2.9279e-05 ,  Y0 :  5.2881e-02 , runtime :  735 \n",
            "step :  3700 , loss : 2.8989e-05 ,  Y0 :  5.2886e-02 , runtime :  758 \n",
            "step :  3800 , loss : 2.8949e-05 ,  Y0 :  5.2904e-02 , runtime :  777 \n",
            "step :  3900 , loss : 2.8876e-05 ,  Y0 :  5.2851e-02 , runtime :  801 \n",
            "step :  4000 , loss : 2.8375e-05 ,  Y0 :  5.2870e-02 , runtime :  820 \n",
            " running time :  820.981 s \n",
            "3 run:\n",
            "Begin to solve Allen - Cahn equation\n",
            "End build\n",
            "step :     0 , loss : 1.6548e-01 , Y0 :  5.4381e-01 , runtime :    7 \n",
            "step :   100 , loss : 1.3029e-01 ,  Y0 :  4.9525e-01 , runtime :   30 \n",
            "step :   200 , loss : 1.0236e-01 ,  Y0 :  4.5037e-01 , runtime :   49 \n",
            "step :   300 , loss : 8.0129e-02 ,  Y0 :  4.0879e-01 , runtime :   67 \n",
            "step :   400 , loss : 6.2943e-02 ,  Y0 :  3.7020e-01 , runtime :   87 \n",
            "step :   500 , loss : 4.9102e-02 ,  Y0 :  3.3438e-01 , runtime :  106 \n",
            "step :   600 , loss : 3.7872e-02 ,  Y0 :  3.0132e-01 , runtime :  127 \n",
            "step :   700 , loss : 2.8862e-02 ,  Y0 :  2.7076e-01 , runtime :  147 \n",
            "step :   800 , loss : 2.1829e-02 ,  Y0 :  2.4276e-01 , runtime :  165 \n",
            "step :   900 , loss : 1.6241e-02 ,  Y0 :  2.1705e-01 , runtime :  184 \n",
            "step :  1000 , loss : 1.1980e-02 ,  Y0 :  1.9373e-01 , runtime :  204 \n",
            "step :  1100 , loss : 8.6807e-03 ,  Y0 :  1.7263e-01 , runtime :  222 \n",
            "step :  1200 , loss : 6.1665e-03 ,  Y0 :  1.5372e-01 , runtime :  243 \n",
            "step :  1300 , loss : 4.2932e-03 ,  Y0 :  1.3690e-01 , runtime :  263 \n",
            "step :  1400 , loss : 2.9790e-03 ,  Y0 :  1.2219e-01 , runtime :  281 \n",
            "step :  1500 , loss : 2.0054e-03 ,  Y0 :  1.0944e-01 , runtime :  302 \n",
            "step :  1600 , loss : 1.3129e-03 ,  Y0 :  9.8402e-02 , runtime :  321 \n",
            "step :  1700 , loss : 8.4904e-04 ,  Y0 :  8.8991e-02 , runtime :  342 \n",
            "step :  1800 , loss : 5.3847e-04 ,  Y0 :  8.1166e-02 , runtime :  361 \n",
            "step :  1900 , loss : 3.3505e-04 ,  Y0 :  7.4690e-02 , runtime :  382 \n",
            "step :  2000 , loss : 2.0952e-04 ,  Y0 :  6.9477e-02 , runtime :  402 \n",
            "step :  2100 , loss : 1.3088e-04 ,  Y0 :  6.5262e-02 , runtime :  422 \n",
            "step :  2200 , loss : 8.5913e-05 ,  Y0 :  6.1955e-02 , runtime :  441 \n",
            "step :  2300 , loss : 6.0006e-05 ,  Y0 :  5.9449e-02 , runtime :  460 \n",
            "step :  2400 , loss : 4.5943e-05 ,  Y0 :  5.7563e-02 , runtime :  480 \n",
            "step :  2500 , loss : 3.7564e-05 ,  Y0 :  5.6156e-02 , runtime :  500 \n",
            "step :  2600 , loss : 3.3322e-05 ,  Y0 :  5.5123e-02 , runtime :  521 \n",
            "step :  2700 , loss : 3.1031e-05 ,  Y0 :  5.4402e-02 , runtime :  542 \n",
            "step :  2800 , loss : 2.9411e-05 ,  Y0 :  5.3867e-02 , runtime :  563 \n",
            "step :  2900 , loss : 2.8986e-05 ,  Y0 :  5.3529e-02 , runtime :  581 \n",
            "step :  3000 , loss : 2.7884e-05 ,  Y0 :  5.3221e-02 , runtime :  602 \n",
            "step :  3100 , loss : 2.7581e-05 ,  Y0 :  5.3093e-02 , runtime :  622 \n",
            "step :  3200 , loss : 2.7543e-05 ,  Y0 :  5.3098e-02 , runtime :  640 \n",
            "step :  3300 , loss : 2.7331e-05 ,  Y0 :  5.3054e-02 , runtime :  661 \n",
            "step :  3400 , loss : 2.7065e-05 ,  Y0 :  5.3004e-02 , runtime :  682 \n",
            "step :  3500 , loss : 2.6788e-05 ,  Y0 :  5.2955e-02 , runtime :  703 \n",
            "step :  3600 , loss : 2.6830e-05 ,  Y0 :  5.2908e-02 , runtime :  721 \n",
            "step :  3700 , loss : 2.6959e-05 ,  Y0 :  5.2924e-02 , runtime :  740 \n",
            "step :  3800 , loss : 2.6473e-05 ,  Y0 :  5.2822e-02 , runtime :  761 \n",
            "step :  3900 , loss : 2.6663e-05 ,  Y0 :  5.2833e-02 , runtime :  780 \n",
            "step :  4000 , loss : 2.6539e-05 ,  Y0 :  5.2837e-02 , runtime :  802 \n",
            " running time :  802.728 s \n",
            "4 run:\n",
            "Begin to solve Allen - Cahn equation\n",
            "End build\n",
            "step :     0 , loss : 1.5621e-01 , Y0 :  5.4381e-01 , runtime :    5 \n",
            "step :   100 , loss : 1.2315e-01 ,  Y0 :  4.9527e-01 , runtime :   28 \n",
            "step :   200 , loss : 9.6976e-02 ,  Y0 :  4.5025e-01 , runtime :   48 \n",
            "step :   300 , loss : 7.6199e-02 ,  Y0 :  4.0861e-01 , runtime :   69 \n",
            "step :   400 , loss : 5.9820e-02 ,  Y0 :  3.7021e-01 , runtime :   87 \n",
            "step :   500 , loss : 4.6509e-02 ,  Y0 :  3.3445e-01 , runtime :  104 \n",
            "step :   600 , loss : 3.6003e-02 ,  Y0 :  3.0146e-01 , runtime :  125 \n",
            "step :   700 , loss : 2.7448e-02 ,  Y0 :  2.7098e-01 , runtime :  144 \n",
            "step :   800 , loss : 2.0612e-02 ,  Y0 :  2.4291e-01 , runtime :  166 \n",
            "step :   900 , loss : 1.5352e-02 ,  Y0 :  2.1718e-01 , runtime :  184 \n",
            "step :  1000 , loss : 1.1253e-02 ,  Y0 :  1.9382e-01 , runtime :  204 \n",
            "step :  1100 , loss : 8.1895e-03 ,  Y0 :  1.7274e-01 , runtime :  222 \n",
            "step :  1200 , loss : 5.8121e-03 ,  Y0 :  1.5384e-01 , runtime :  240 \n",
            "step :  1300 , loss : 4.0644e-03 ,  Y0 :  1.3711e-01 , runtime :  260 \n",
            "step :  1400 , loss : 2.7672e-03 ,  Y0 :  1.2235e-01 , runtime :  278 \n",
            "step :  1500 , loss : 1.8434e-03 ,  Y0 :  1.0945e-01 , runtime :  302 \n",
            "step :  1600 , loss : 1.1932e-03 ,  Y0 :  9.8389e-02 , runtime :  320 \n",
            "step :  1700 , loss : 7.6403e-04 ,  Y0 :  8.9010e-02 , runtime :  339 \n",
            "step :  1800 , loss : 4.8266e-04 ,  Y0 :  8.1187e-02 , runtime :  359 \n",
            "step :  1900 , loss : 2.9953e-04 ,  Y0 :  7.4724e-02 , runtime :  378 \n",
            "step :  2000 , loss : 1.8330e-04 ,  Y0 :  6.9465e-02 , runtime :  400 \n",
            "step :  2100 , loss : 1.1589e-04 ,  Y0 :  6.5270e-02 , runtime :  420 \n",
            "step :  2200 , loss : 7.3858e-05 ,  Y0 :  6.1956e-02 , runtime :  442 \n",
            "step :  2300 , loss : 5.1412e-05 ,  Y0 :  5.9442e-02 , runtime :  461 \n",
            "step :  2400 , loss : 3.9115e-05 ,  Y0 :  5.7522e-02 , runtime :  482 \n",
            "step :  2500 , loss : 3.3033e-05 ,  Y0 :  5.6060e-02 , runtime :  500 \n",
            "step :  2600 , loss : 3.0012e-05 ,  Y0 :  5.4999e-02 , runtime :  520 \n",
            "step :  2700 , loss : 2.8459e-05 ,  Y0 :  5.4282e-02 , runtime :  539 \n",
            "step :  2800 , loss : 2.7984e-05 ,  Y0 :  5.3795e-02 , runtime :  557 \n",
            "step :  2900 , loss : 2.7087e-05 ,  Y0 :  5.3457e-02 , runtime :  580 \n",
            "step :  3000 , loss : 2.6955e-05 ,  Y0 :  5.3255e-02 , runtime :  599 \n",
            "step :  3100 , loss : 2.6474e-05 ,  Y0 :  5.3151e-02 , runtime :  620 \n",
            "step :  3200 , loss : 2.5893e-05 ,  Y0 :  5.3027e-02 , runtime :  638 \n",
            "step :  3300 , loss : 2.5808e-05 ,  Y0 :  5.3036e-02 , runtime :  657 \n",
            "step :  3400 , loss : 2.5933e-05 ,  Y0 :  5.2974e-02 , runtime :  677 \n",
            "step :  3500 , loss : 2.5562e-05 ,  Y0 :  5.2928e-02 , runtime :  696 \n",
            "step :  3600 , loss : 2.5678e-05 ,  Y0 :  5.2958e-02 , runtime :  720 \n",
            "step :  3700 , loss : 2.5765e-05 ,  Y0 :  5.2900e-02 , runtime :  737 \n",
            "step :  3800 , loss : 2.5657e-05 ,  Y0 :  5.2886e-02 , runtime :  757 \n",
            "step :  3900 , loss : 2.5624e-05 ,  Y0 :  5.2866e-02 , runtime :  778 \n",
            "step :  4000 , loss : 2.5742e-05 ,  Y0 :  5.2916e-02 , runtime :  796 \n",
            " running time :  796.653 s \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average  # Moving average\n",
        "from scipy.stats import multivariate_normal as normal  # Generate normally distributed random numbers\n",
        "from tensorflow.python.ops import control_flow_ops  # Used for control flow\n",
        "from tensorflow import random_normal_initializer as norm_init  # Tensor initializer with normal distribution\n",
        "from tensorflow import random_uniform_initializer as unif_init  # Tensor initializer with uniform distribution\n",
        "from tensorflow import constant_initializer as const_init  # Tensor initializer with constant values\n",
        "from tensorflow.keras.layers import LSTMCell\n",
        "\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class SolveAllenCahn(object):\n",
        "    \"\"\" The fully-connected neural network model \"\"\"\n",
        "    def __init__(self, sess):\n",
        "        self.sess = sess  # Session\n",
        "        # parameters for the PDE\n",
        "        self.d = 100  # Dimension of the data\n",
        "        self.T = 0.3  # Time length for each path\n",
        "        # parameters for the algorithm\n",
        "        self.n_time = 20  # Number of networks\n",
        "        self.batch_size = 64  # Use paths in batches for calculations, 64*4=256\n",
        "        self.valid_size = 256  # 256 Monte Carlo samples (paths)\n",
        "        self.n_maxstep = 4000  # Iteration steps\n",
        "        self.n_displaystep = 100  # Print every 100 steps\n",
        "        self.learning_rate = 5e-4  # Learning rate\n",
        "        self.Yini = [0.3,0.6]  # Min and max values for initial Y0\n",
        "        # some basic constants and variables\n",
        "        self.h = (self.T + 0.0) / self.n_time  # Data time interval for each path, delta t\n",
        "        self.sqrth = math.sqrt(self.h)  # sqrt(delta t), used later in calculations\n",
        "        self.t_stamp = np.arange(0, self.n_time) * self.h  # Time stamps, cumulative time\n",
        "        self._extra_train_ops = []  # Batch moving average operation, includes additional trainable beta and gamma\n",
        "\n",
        "    def train(self):\n",
        "        # Main function for neural network training\n",
        "        start_time = time.time()  # Start time\n",
        "        # Create new TensorFlow variable, name='global_step', as a generator for consistency\n",
        "        self.global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(1),\n",
        "                                           trainable=False, dtype=tf.int32)  # Not added to the trainable variable list, just a counter\n",
        "        trainable_vars = tf.trainable_variables()  # View trainable variables\n",
        "        grads = tf.gradients(self.loss, trainable_vars)  # Gradients for loss trainable variables\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)  # Gradient optimizer\n",
        "        apply_op = optimizer.apply_gradients(zip(grads, trainable_vars),  # Apply gradients to update trainable_vars list\n",
        "                                             global_step=self.global_step)  # Update gradients and iteration count\n",
        "\n",
        "        train_ops = [apply_op] + self._extra_train_ops  # Add operations, similar to list1.extend(list2)\n",
        "        self.train_op = tf.group(*train_ops)  # tf.group(*train_ops) combines operations in *train_ops\n",
        "\n",
        "        self.loss_history = []  # To record loss values\n",
        "        self.init_history = []  # To record Y0 values\n",
        "        self.run_time = []  # To record runtime\n",
        "\n",
        "        # For validation, 256 Monte Carlo samples as the validation set\n",
        "        dW_valid, X_valid = self.sample_path(self.valid_size)  # Generate data\n",
        "        feed_dict_valid = {self.dW: dW_valid,  # Feed data to placeholders in buildmodel\n",
        "                           self.X: X_valid,\n",
        "                           self.is_training: False}  # Exclude from iteration\n",
        "        # Initialization\n",
        "        step = 1\n",
        "        self.sess.run(tf.global_variables_initializer())  # Initialize global variables\n",
        "\n",
        "        # Run framework\n",
        "        temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)  # Calculate loss\n",
        "\n",
        "        temp_init = self.Y0.eval()[0]  # Extract values, Y0 is a 2D tensor\n",
        "        self.loss_history.append(temp_loss)  # Record loss\n",
        "        self.init_history.append(temp_init)  # Record Y0\n",
        "        self.run_time.append(time.time() - start_time + self.t_bd)\n",
        "        print(\"step : %5u , loss : %.4e , \" % \\\n",
        "              (0, temp_loss) + \\\n",
        "              \"Y0 : % .4e , runtime : %4u \" % \\\n",
        "              (temp_init, time.time() - start_time + self.t_bd))  # Print state for step=0\n",
        "\n",
        "        # Start SGD iteration, steps 0-4000\n",
        "        for _ in range(self.n_maxstep + 1):\n",
        "            step = self.sess.run(self.global_step)\n",
        "            dW_train, X_train = self.sample_path(self.batch_size)  # Generate data\n",
        "            self.sess.run(self.train_op,\n",
        "                          feed_dict={self.dW: dW_train,  # Feed data to placeholders in buildmodel\n",
        "                                     self.X: X_train,\n",
        "                                     self.is_training: True})\n",
        "            if step % self.n_displaystep == 0:  # Test with validation set every 100 steps for loss and Y0\n",
        "                temp_loss = self.sess.run(self.loss, feed_dict=feed_dict_valid)\n",
        "                temp_init = self.Y0.eval()[0]  # Extract values\n",
        "                self.loss_history.append(temp_loss)  # Loss values\n",
        "                self.init_history.append(temp_init)  # Y0 values, Y0 is a 2D tensor\n",
        "                self.run_time.append(time.time() - start_time + self.t_bd)\n",
        "                print(\"step : % 5u , loss : %.4e , \" % \\\n",
        "                      (step, temp_loss) + \\\n",
        "                      \" Y0 : % .4e , runtime : %4u \" % \\\n",
        "                      (temp_init, time.time() - start_time + self.t_bd))\n",
        "            step += 1\n",
        "        end_time = time.time()  # End time for training\n",
        "        print(\" running time : % .3f s \" % \\\n",
        "              (end_time - start_time + self.t_bd))\n",
        "\n",
        "    def build(self):\n",
        "        # Build the whole network by stacking subnetworks\n",
        "        start_time = time.time()\n",
        "        # Placeholders for dW, X, and is_training, batch size is None to process one batch at a time\n",
        "        self.dW = tf.placeholder(tf.float32, [None, self.d, self.n_time], name='dW')  # None*100*20\n",
        "        self.X = tf.placeholder(tf.float32, [None, self.d, self.n_time + 1], name='X')  # None*100*20\n",
        "        self.is_training = tf.placeholder(tf.bool)\n",
        "\n",
        "        # Initialize Y0 and Z0\n",
        "        self.Y0 = tf.Variable(tf.random_uniform([1],  # Initial u0, one value for one dimension\n",
        "                                                minval=self.Yini[0],  # Min value 0.3\n",
        "                                                maxval=self.Yini[1],  # Max value 0.6\n",
        "                                                dtype=tf.float32))\n",
        "        self.Z0 = tf.Variable(tf.random_uniform([1, self.d],  # Initial value for gradient u, a 1*d vector\n",
        "                                                minval=-.1,  # Min value\n",
        "                                                maxval=.1,  # Max value\n",
        "                                                dtype=tf.float32))\n",
        "        self.allones = tf.ones(shape=tf.stack([tf.shape(self.dW)[0], 1]),  # tf.shape(self.dW)[0]=len(batch), shape=(batch,1)\n",
        "                               dtype=tf.float32)  # Batch generation of initial values\n",
        "\n",
        "        Y = self.allones * self.Y0  # Initial Y as input, each batch gets the same initial Y value, Y is a (batch,1) 2D matrix [[],[],..,]\n",
        "        Z = tf.matmul(self.allones, self.Z0)  # Initial Z as output, similar to Y, but as Z is a vector, it’s a (batch, d) matrix\n",
        "\n",
        "        with tf.variable_scope('forward', reuse=tf.AUTO_REUSE):  # Forward propagation\n",
        "            cell = LSTMCell(units=110)\n",
        "            batch_size = tf.shape(self.X)[0]\n",
        "            hid = [tf.zeros([batch_size, cell.units]), tf.zeros([batch_size, cell.units])]\n",
        "            w = tf.get_variable('Matrixhid',\n",
        "                                [110, 100], tf.float32,\n",
        "                                norm_init(stddev=5 / np.sqrt(110)))\n",
        "            for t in range(0, self.n_time - 1):  # Network for the first N-1 xt\n",
        "                # Computation from recursive formula\n",
        "                Y = Y - self.f_tf(self.t_stamp[t],  # Timestamp, cumulative time value\n",
        "                                  self.X[:, :, t], Y, Z) * self.h  # Y.shape=(batch,1)\n",
        "                Y = Y + tf.reduce_sum(Z * self.dW[:, :, t], 1, keep_dims=True)  # Intermediate time output.\n",
        "                output, hid = cell(self._batch_norm(self.X[:, :, t + 1], 'normal'), hid)  # Update hid at each time\n",
        "                Z = tf.nn.relu(tf.matmul(output, w)) / self.d\n",
        "\n",
        "            # Terminal time, as final Y does not use neural network\n",
        "            Y = Y - self.f_tf(self.t_stamp[self.n_time - 1],  # -1 because index starts from 0, terminal step is different\n",
        "                              self.X[:, :, self.n_time - 1], Y, Z) * self.h\n",
        "            Y = Y + tf.reduce_sum(Z * self.dW[:, :, self.n_time - 1], 1, keep_dims=True)  # Final Y output\n",
        "            term_delta = Y - self.g_tf(self.T, self.X[:, :, self.n_time])  # Loss function\n",
        "            self.clipped_delta = tf.clip_by_value(term_delta, -50.0, 50.0)  # Clip values within bounds\n",
        "            self.loss = tf.reduce_mean(self.clipped_delta ** 2)  # Calculate loss\n",
        "        self.t_bd = time.time() - start_time  # Time to generate the network\n",
        "\n",
        "    def sample_path(self, n_sample):\n",
        "        # Generate paths, creating (xt, (wt-wt-1))\n",
        "        dW_sample = np.zeros([n_sample, self.d, self.n_time])  # Sample count, dimension, time length\n",
        "        X_sample = np.zeros([n_sample, self.d, self.n_time + 1])\n",
        "        for i in range(self.n_time):  # Generate one column at a time\n",
        "            dW_sample[:, :, i] = np.reshape(normal.rvs(mean=np.zeros(self.d),  # This function is similar to np.random.normal()\n",
        "                                                       cov=1,  # Why not std=1 ?\n",
        "                                                       size=n_sample) * self.sqrth,  # sqrt(delta t), W(t)-W(s) is independent, with mean 0 and variance t-s\n",
        "                                              (n_sample, self.d))\n",
        "            X_sample[:, :, i + 1] = X_sample[:, :, i] + np.sqrt(2) * dW_sample[:, :, i]  # From formula\n",
        "        return dW_sample, X_sample\n",
        "\n",
        "    def f_tf(self, t, X, Y, Z):\n",
        "        # Nonlinear term\n",
        "        return Y - tf.pow(Y, 3)\n",
        "\n",
        "    def g_tf(self, t, X):\n",
        "        # Terminal conditions\n",
        "        return 0.5 / (1 + 0.2 * tf.reduce_sum(X ** 2, 1, keep_dims=True))\n",
        "\n",
        "    def _batch_norm(self, x, name):\n",
        "        \"\"\" Batch normalization \"\"\"  # Beta and gamma are trainable, third type of parameter, needs 2 columns for each normalization\n",
        "        with tf.variable_scope(name):\n",
        "            params_shape = [x.get_shape()[-1]]  # [d, d+10, d+10, d], first dimension is batch\n",
        "            beta = tf.get_variable('beta', params_shape, tf.float32, norm_init(0.0, stddev=0.1))\n",
        "            gamma = tf.get_variable('gamma', params_shape, tf.float32, unif_init(0.1, 0.5))\n",
        "            mv_mean = tf.get_variable('moving_mean',  # Moving_mean improves mean for different batches\n",
        "                                      params_shape, tf.float32, const_init(0.0), trainable=False)\n",
        "            mv_var = tf.get_variable('moving_variance',\n",
        "                                     params_shape, tf.float32, const_init(1.0), trainable=False)\n",
        "\n",
        "            # These ops will only be performed when training\n",
        "            mean, variance = tf.nn.moments(x, [0], name='moments')  # Centered dimension for normalization, [0] means batch, calculate mean and variance for 64 samples\n",
        "            self._extra_train_ops.append(assign_moving_average(mv_mean, mean, 0.99))  # Explained below\n",
        "            self._extra_train_ops.append(assign_moving_average(mv_var, variance, 0.99))\n",
        "\n",
        "            mean, variance = tf.cond(self.is_training,  # control_flow_ops.cond controls execution flow, first parameter is the condition\n",
        "                                                   lambda: (mean, variance),  # If True, calculate mean and variance\n",
        "                                                   lambda: (mv_mean, mv_var))  # If False, directly use smoothed value\n",
        "\n",
        "            y = tf.nn.batch_normalization(x, mean, variance, beta, gamma, 1e-6)\n",
        "            # Above operation is equivalent to:\n",
        "            # y = (y - mean) / tf.sqrt(variance+1e-6)  # 1e-6 epsilon\n",
        "            # y = y * gamma + beta\n",
        "\n",
        "            # Ensure that shape is maintained after normalization\n",
        "            y.set_shape(x.get_shape())\n",
        "            return y\n",
        "\n",
        "\n",
        "def main(name):\n",
        "    tf.reset_default_graph()\n",
        "    with tf.Session() as sess:\n",
        "        tf.set_random_seed(1)  # Random seed in tf\n",
        "        print(\"Begin to solve Allen - Cahn equation\")\n",
        "        model = SolveAllenCahn(sess)  # Create object\n",
        "        model.build()  # Call object method, constructs a model but does not feed data\n",
        "        print('End build')\n",
        "        model.train()  # Generate and feed data into build\n",
        "        output = np.zeros((len(model.init_history), 4))  # Initialize result as 0, fill later\n",
        "        output[:, 0] = np.arange(len(model.init_history)) * model.n_displaystep  # Output step\n",
        "        output[:, 1] = model.loss_history  # Output loss list\n",
        "        output[:, 2] = model.init_history  # Output Y0 list\n",
        "        output[:, 3] = model.run_time\n",
        "        np.savetxt(\"./AllenCahn_d100_RNN\" + str(name) + \".csv\",  # Save output results\n",
        "                   output,\n",
        "                   fmt=['%d', '%.5e', '%.5e', '%d'],\n",
        "                   delimiter=\",\",\n",
        "                   header=\"step ,loss function , target value , runtime \",\n",
        "                   comments='')\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    np.random.seed(1)  # Define a random seed\n",
        "    for i in range(5):\n",
        "        print(str(i) + ' run:')\n",
        "        main(i)  # Run main program\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

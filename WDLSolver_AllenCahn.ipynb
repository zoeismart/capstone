{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkEQbs8AJwWH"
      },
      "source": [
        "##### WLD SOLVER"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyEEHbtd7dJx",
        "outputId": "91da3ff0-ac6a-4444-c534-2833a45f0ca7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iIDPN9CBJwWP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d308edfc-c8cb-4eac-da35-b1de6fe876bf"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/compat/v2_compat.py:98: disable_resource_variables (from tensorflow.python.ops.resource_variables_toggle) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/util/dispatch.py:1260: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "keep_dims is deprecated, use keepdims instead\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6158e-01 , Y0 :  5.4381e-01 , runtime :   16 \n",
            "step :   100 , loss : 1.2692e-01 ,  Y0 :  4.9516e-01 , runtime :   44 \n",
            "step :   200 , loss : 9.9422e-02 ,  Y0 :  4.5004e-01 , runtime :   63 \n",
            "step :   300 , loss : 7.8032e-02 ,  Y0 :  4.0832e-01 , runtime :   84 \n",
            "step :   400 , loss : 6.0838e-02 ,  Y0 :  3.6975e-01 , runtime :  107 \n",
            "step :   500 , loss : 4.7267e-02 ,  Y0 :  3.3401e-01 , runtime :  130 \n",
            "step :   600 , loss : 3.6247e-02 ,  Y0 :  3.0103e-01 , runtime :  150 \n",
            "step :   700 , loss : 2.7612e-02 ,  Y0 :  2.7048e-01 , runtime :  168 \n",
            "step :   800 , loss : 2.0776e-02 ,  Y0 :  2.4241e-01 , runtime :  190 \n",
            "step :   900 , loss : 1.5425e-02 ,  Y0 :  2.1670e-01 , runtime :  209 \n",
            "step :  1000 , loss : 1.1287e-02 ,  Y0 :  1.9333e-01 , runtime :  226 \n",
            "step :  1100 , loss : 8.1870e-03 ,  Y0 :  1.7231e-01 , runtime :  246 \n",
            "step :  1200 , loss : 5.8513e-03 ,  Y0 :  1.5347e-01 , runtime :  266 \n",
            "step :  1300 , loss : 4.1319e-03 ,  Y0 :  1.3674e-01 , runtime :  287 \n",
            "step :  1400 , loss : 2.8761e-03 ,  Y0 :  1.2206e-01 , runtime :  302 \n",
            "step :  1500 , loss : 1.9987e-03 ,  Y0 :  1.0931e-01 , runtime :  322 \n",
            "step :  1600 , loss : 1.3636e-03 ,  Y0 :  9.8167e-02 , runtime :  338 \n",
            "step :  1700 , loss : 9.4600e-04 ,  Y0 :  8.8811e-02 , runtime :  358 \n",
            "step :  1800 , loss : 6.7112e-04 ,  Y0 :  8.1034e-02 , runtime :  376 \n",
            "step :  1900 , loss : 4.9303e-04 ,  Y0 :  7.4495e-02 , runtime :  397 \n",
            "step :  2000 , loss : 3.8376e-04 ,  Y0 :  6.9254e-02 , runtime :  418 \n",
            "step :  2100 , loss : 3.2243e-04 ,  Y0 :  6.5190e-02 , runtime :  437 \n",
            "step :  2200 , loss : 2.8482e-04 ,  Y0 :  6.1910e-02 , runtime :  454 \n",
            "step :  2300 , loss : 2.6494e-04 ,  Y0 :  5.9353e-02 , runtime :  471 \n",
            "step :  2400 , loss : 2.5307e-04 ,  Y0 :  5.7424e-02 , runtime :  489 \n",
            "step :  2500 , loss : 2.4721e-04 ,  Y0 :  5.6086e-02 , runtime :  506 \n",
            "step :  2600 , loss : 2.4428e-04 ,  Y0 :  5.5164e-02 , runtime :  525 \n",
            "step :  2700 , loss : 2.3603e-04 ,  Y0 :  5.4373e-02 , runtime :  545 \n",
            "step :  2800 , loss : 2.3407e-04 ,  Y0 :  5.3833e-02 , runtime :  565 \n",
            "step :  2900 , loss : 2.3577e-04 ,  Y0 :  5.3552e-02 , runtime :  585 \n",
            "step :  3000 , loss : 2.3451e-04 ,  Y0 :  5.3180e-02 , runtime :  600 \n",
            "step :  3100 , loss : 2.3076e-04 ,  Y0 :  5.3086e-02 , runtime :  619 \n",
            "step :  3200 , loss : 2.2866e-04 ,  Y0 :  5.3076e-02 , runtime :  636 \n",
            "step :  3300 , loss : 2.2528e-04 ,  Y0 :  5.2934e-02 , runtime :  656 \n",
            "step :  3400 , loss : 2.2210e-04 ,  Y0 :  5.3084e-02 , runtime :  677 \n",
            "step :  3500 , loss : 2.1987e-04 ,  Y0 :  5.3002e-02 , runtime :  696 \n",
            "step :  3600 , loss : 2.1868e-04 ,  Y0 :  5.3008e-02 , runtime :  714 \n",
            "step :  3700 , loss : 2.1313e-04 ,  Y0 :  5.3000e-02 , runtime :  735 \n",
            "step :  3800 , loss : 2.1229e-04 ,  Y0 :  5.2903e-02 , runtime :  759 \n",
            "step :  3900 , loss : 2.0669e-04 ,  Y0 :  5.2922e-02 , runtime :  776 \n",
            "step :  4000 , loss : 1.9803e-04 ,  Y0 :  5.3184e-02 , runtime :  793 \n",
            " running time :  793.196 s \n",
            "1 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.5930e-01 , Y0 :  5.4381e-01 , runtime :   13 \n",
            "step :   100 , loss : 1.2452e-01 ,  Y0 :  4.9533e-01 , runtime :   40 \n",
            "step :   200 , loss : 9.7679e-02 ,  Y0 :  4.5050e-01 , runtime :   61 \n",
            "step :   300 , loss : 7.6796e-02 ,  Y0 :  4.0891e-01 , runtime :   82 \n",
            "step :   400 , loss : 6.0121e-02 ,  Y0 :  3.7022e-01 , runtime :  103 \n",
            "step :   500 , loss : 4.6720e-02 ,  Y0 :  3.3442e-01 , runtime :  122 \n",
            "step :   600 , loss : 3.5900e-02 ,  Y0 :  3.0137e-01 , runtime :  143 \n",
            "step :   700 , loss : 2.7508e-02 ,  Y0 :  2.7090e-01 , runtime :  164 \n",
            "step :   800 , loss : 2.0774e-02 ,  Y0 :  2.4277e-01 , runtime :  185 \n",
            "step :   900 , loss : 1.5535e-02 ,  Y0 :  2.1718e-01 , runtime :  206 \n",
            "step :  1000 , loss : 1.1440e-02 ,  Y0 :  1.9381e-01 , runtime :  226 \n",
            "step :  1100 , loss : 8.2972e-03 ,  Y0 :  1.7261e-01 , runtime :  245 \n",
            "step :  1200 , loss : 5.9262e-03 ,  Y0 :  1.5379e-01 , runtime :  264 \n",
            "step :  1300 , loss : 4.1974e-03 ,  Y0 :  1.3695e-01 , runtime :  286 \n",
            "step :  1400 , loss : 2.9346e-03 ,  Y0 :  1.2219e-01 , runtime :  303 \n",
            "step :  1500 , loss : 2.0296e-03 ,  Y0 :  1.0936e-01 , runtime :  326 \n",
            "step :  1600 , loss : 1.4085e-03 ,  Y0 :  9.8318e-02 , runtime :  342 \n",
            "step :  1700 , loss : 9.8654e-04 ,  Y0 :  8.8880e-02 , runtime :  360 \n",
            "step :  1800 , loss : 7.1163e-04 ,  Y0 :  8.1026e-02 , runtime :  378 \n",
            "step :  1900 , loss : 5.3148e-04 ,  Y0 :  7.4552e-02 , runtime :  401 \n",
            "step :  2000 , loss : 4.1823e-04 ,  Y0 :  6.9285e-02 , runtime :  423 \n",
            "step :  2100 , loss : 3.5359e-04 ,  Y0 :  6.5125e-02 , runtime :  443 \n",
            "step :  2200 , loss : 3.1143e-04 ,  Y0 :  6.1826e-02 , runtime :  463 \n",
            "step :  2300 , loss : 2.9074e-04 ,  Y0 :  5.9362e-02 , runtime :  486 \n",
            "step :  2400 , loss : 2.7995e-04 ,  Y0 :  5.7297e-02 , runtime :  505 \n",
            "step :  2500 , loss : 2.7489e-04 ,  Y0 :  5.5945e-02 , runtime :  527 \n",
            "step :  2600 , loss : 2.6931e-04 ,  Y0 :  5.5077e-02 , runtime :  547 \n",
            "step :  2700 , loss : 2.6571e-04 ,  Y0 :  5.4382e-02 , runtime :  564 \n",
            "step :  2800 , loss : 2.6410e-04 ,  Y0 :  5.3791e-02 , runtime :  582 \n",
            "step :  2900 , loss : 2.6204e-04 ,  Y0 :  5.3385e-02 , runtime :  602 \n",
            "step :  3000 , loss : 2.6214e-04 ,  Y0 :  5.3317e-02 , runtime :  621 \n",
            "step :  3100 , loss : 2.5646e-04 ,  Y0 :  5.3162e-02 , runtime :  639 \n",
            "step :  3200 , loss : 2.5464e-04 ,  Y0 :  5.3171e-02 , runtime :  658 \n",
            "step :  3300 , loss : 2.5293e-04 ,  Y0 :  5.2863e-02 , runtime :  680 \n",
            "step :  3400 , loss : 2.4946e-04 ,  Y0 :  5.2963e-02 , runtime :  697 \n",
            "step :  3500 , loss : 2.4460e-04 ,  Y0 :  5.2924e-02 , runtime :  715 \n",
            "step :  3600 , loss : 2.3926e-04 ,  Y0 :  5.2896e-02 , runtime :  734 \n",
            "step :  3700 , loss : 2.3767e-04 ,  Y0 :  5.2717e-02 , runtime :  754 \n",
            "step :  3800 , loss : 2.3469e-04 ,  Y0 :  5.2642e-02 , runtime :  772 \n",
            "step :  3900 , loss : 2.3060e-04 ,  Y0 :  5.2787e-02 , runtime :  790 \n",
            "step :  4000 , loss : 2.2533e-04 ,  Y0 :  5.3060e-02 , runtime :  811 \n",
            " running time :  811.969 s \n",
            "2 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6276e-01 , Y0 :  5.4381e-01 , runtime :   13 \n",
            "step :   100 , loss : 1.2756e-01 ,  Y0 :  4.9525e-01 , runtime :   40 \n",
            "step :   200 , loss : 1.0019e-01 ,  Y0 :  4.5039e-01 , runtime :   60 \n",
            "step :   300 , loss : 7.8726e-02 ,  Y0 :  4.0885e-01 , runtime :   83 \n",
            "step :   400 , loss : 6.1394e-02 ,  Y0 :  3.7021e-01 , runtime :  104 \n",
            "step :   500 , loss : 4.7542e-02 ,  Y0 :  3.3439e-01 , runtime :  120 \n",
            "step :   600 , loss : 3.6464e-02 ,  Y0 :  3.0116e-01 , runtime :  141 \n",
            "step :   700 , loss : 2.7747e-02 ,  Y0 :  2.7067e-01 , runtime :  160 \n",
            "step :   800 , loss : 2.0873e-02 ,  Y0 :  2.4263e-01 , runtime :  182 \n",
            "step :   900 , loss : 1.5542e-02 ,  Y0 :  2.1686e-01 , runtime :  198 \n",
            "step :  1000 , loss : 1.1410e-02 ,  Y0 :  1.9348e-01 , runtime :  217 \n",
            "step :  1100 , loss : 8.2315e-03 ,  Y0 :  1.7232e-01 , runtime :  237 \n",
            "step :  1200 , loss : 5.8719e-03 ,  Y0 :  1.5338e-01 , runtime :  255 \n",
            "step :  1300 , loss : 4.1259e-03 ,  Y0 :  1.3658e-01 , runtime :  274 \n",
            "step :  1400 , loss : 2.8735e-03 ,  Y0 :  1.2190e-01 , runtime :  298 \n",
            "step :  1500 , loss : 1.9692e-03 ,  Y0 :  1.0910e-01 , runtime :  315 \n",
            "step :  1600 , loss : 1.3650e-03 ,  Y0 :  9.8080e-02 , runtime :  336 \n",
            "step :  1700 , loss : 9.5956e-04 ,  Y0 :  8.8824e-02 , runtime :  356 \n",
            "step :  1800 , loss : 6.9267e-04 ,  Y0 :  8.0929e-02 , runtime :  376 \n",
            "step :  1900 , loss : 5.2481e-04 ,  Y0 :  7.4504e-02 , runtime :  395 \n",
            "step :  2000 , loss : 4.2625e-04 ,  Y0 :  6.9261e-02 , runtime :  413 \n",
            "step :  2100 , loss : 3.6892e-04 ,  Y0 :  6.5170e-02 , runtime :  435 \n",
            "step :  2200 , loss : 3.3460e-04 ,  Y0 :  6.1897e-02 , runtime :  456 \n",
            "step :  2300 , loss : 3.1519e-04 ,  Y0 :  5.9338e-02 , runtime :  476 \n",
            "step :  2400 , loss : 3.0789e-04 ,  Y0 :  5.7517e-02 , runtime :  495 \n",
            "step :  2500 , loss : 3.0532e-04 ,  Y0 :  5.6134e-02 , runtime :  513 \n",
            "step :  2600 , loss : 3.0353e-04 ,  Y0 :  5.5057e-02 , runtime :  532 \n",
            "step :  2700 , loss : 2.9970e-04 ,  Y0 :  5.4486e-02 , runtime :  549 \n",
            "step :  2800 , loss : 2.9841e-04 ,  Y0 :  5.3902e-02 , runtime :  568 \n",
            "step :  2900 , loss : 2.9307e-04 ,  Y0 :  5.3464e-02 , runtime :  586 \n",
            "step :  3000 , loss : 2.8888e-04 ,  Y0 :  5.3260e-02 , runtime :  603 \n",
            "step :  3100 , loss : 2.8743e-04 ,  Y0 :  5.3156e-02 , runtime :  623 \n",
            "step :  3200 , loss : 2.8344e-04 ,  Y0 :  5.3237e-02 , runtime :  641 \n",
            "step :  3300 , loss : 2.8068e-04 ,  Y0 :  5.3015e-02 , runtime :  660 \n",
            "step :  3400 , loss : 2.7258e-04 ,  Y0 :  5.3012e-02 , runtime :  681 \n",
            "step :  3500 , loss : 2.7045e-04 ,  Y0 :  5.2779e-02 , runtime :  702 \n",
            "step :  3600 , loss : 2.6904e-04 ,  Y0 :  5.2550e-02 , runtime :  723 \n",
            "step :  3700 , loss : 2.6778e-04 ,  Y0 :  5.2656e-02 , runtime :  746 \n",
            "step :  3800 , loss : 2.6487e-04 ,  Y0 :  5.2908e-02 , runtime :  765 \n",
            "step :  3900 , loss : 2.5852e-04 ,  Y0 :  5.3034e-02 , runtime :  787 \n",
            "step :  4000 , loss : 2.4926e-04 ,  Y0 :  5.3025e-02 , runtime :  803 \n",
            " running time :  803.293 s \n",
            "3 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.6503e-01 , Y0 :  5.4381e-01 , runtime :   14 \n",
            "step :   100 , loss : 1.2897e-01 ,  Y0 :  4.9519e-01 , runtime :   38 \n",
            "step :   200 , loss : 1.0115e-01 ,  Y0 :  4.5028e-01 , runtime :   58 \n",
            "step :   300 , loss : 7.9315e-02 ,  Y0 :  4.0871e-01 , runtime :   79 \n",
            "step :   400 , loss : 6.2282e-02 ,  Y0 :  3.7011e-01 , runtime :   98 \n",
            "step :   500 , loss : 4.8511e-02 ,  Y0 :  3.3435e-01 , runtime :  117 \n",
            "step :   600 , loss : 3.7477e-02 ,  Y0 :  3.0131e-01 , runtime :  137 \n",
            "step :   700 , loss : 2.8558e-02 ,  Y0 :  2.7072e-01 , runtime :  158 \n",
            "step :   800 , loss : 2.1567e-02 ,  Y0 :  2.4268e-01 , runtime :  179 \n",
            "step :   900 , loss : 1.6143e-02 ,  Y0 :  2.1700e-01 , runtime :  200 \n",
            "step :  1000 , loss : 1.1925e-02 ,  Y0 :  1.9363e-01 , runtime :  215 \n",
            "step :  1100 , loss : 8.6379e-03 ,  Y0 :  1.7249e-01 , runtime :  237 \n",
            "step :  1200 , loss : 6.1953e-03 ,  Y0 :  1.5363e-01 , runtime :  254 \n",
            "step :  1300 , loss : 4.3932e-03 ,  Y0 :  1.3682e-01 , runtime :  275 \n",
            "step :  1400 , loss : 3.0883e-03 ,  Y0 :  1.2204e-01 , runtime :  295 \n",
            "step :  1500 , loss : 2.1647e-03 ,  Y0 :  1.0931e-01 , runtime :  314 \n",
            "step :  1600 , loss : 1.5022e-03 ,  Y0 :  9.8287e-02 , runtime :  334 \n",
            "step :  1700 , loss : 1.0576e-03 ,  Y0 :  8.8954e-02 , runtime :  351 \n",
            "step :  1800 , loss : 7.5016e-04 ,  Y0 :  8.1072e-02 , runtime :  371 \n",
            "step :  1900 , loss : 5.5528e-04 ,  Y0 :  7.4568e-02 , runtime :  391 \n",
            "step :  2000 , loss : 4.3170e-04 ,  Y0 :  6.9401e-02 , runtime :  410 \n",
            "step :  2100 , loss : 3.5773e-04 ,  Y0 :  6.5311e-02 , runtime :  426 \n",
            "step :  2200 , loss : 3.0932e-04 ,  Y0 :  6.1969e-02 , runtime :  446 \n",
            "step :  2300 , loss : 2.8314e-04 ,  Y0 :  5.9554e-02 , runtime :  465 \n",
            "step :  2400 , loss : 2.6716e-04 ,  Y0 :  5.7556e-02 , runtime :  483 \n",
            "step :  2500 , loss : 2.5660e-04 ,  Y0 :  5.6142e-02 , runtime :  503 \n",
            "step :  2600 , loss : 2.5015e-04 ,  Y0 :  5.5089e-02 , runtime :  523 \n",
            "step :  2700 , loss : 2.4861e-04 ,  Y0 :  5.4471e-02 , runtime :  543 \n",
            "step :  2800 , loss : 2.4434e-04 ,  Y0 :  5.3898e-02 , runtime :  563 \n",
            "step :  2900 , loss : 2.4110e-04 ,  Y0 :  5.3596e-02 , runtime :  583 \n",
            "step :  3000 , loss : 2.3887e-04 ,  Y0 :  5.3344e-02 , runtime :  605 \n",
            "step :  3100 , loss : 2.3629e-04 ,  Y0 :  5.3116e-02 , runtime :  624 \n",
            "step :  3200 , loss : 2.3345e-04 ,  Y0 :  5.3171e-02 , runtime :  645 \n",
            "step :  3300 , loss : 2.3119e-04 ,  Y0 :  5.2962e-02 , runtime :  665 \n",
            "step :  3400 , loss : 2.2717e-04 ,  Y0 :  5.2908e-02 , runtime :  684 \n",
            "step :  3500 , loss : 2.2890e-04 ,  Y0 :  5.2796e-02 , runtime :  705 \n",
            "step :  3600 , loss : 2.2320e-04 ,  Y0 :  5.2896e-02 , runtime :  722 \n",
            "step :  3700 , loss : 2.1831e-04 ,  Y0 :  5.2740e-02 , runtime :  742 \n",
            "step :  3800 , loss : 2.1668e-04 ,  Y0 :  5.2883e-02 , runtime :  761 \n",
            "step :  3900 , loss : 2.1210e-04 ,  Y0 :  5.2747e-02 , runtime :  777 \n",
            "step :  4000 , loss : 2.0992e-04 ,  Y0 :  5.2706e-02 , runtime :  798 \n",
            " running time :  798.215 s \n",
            "4 run:\n",
            " Begin to solve Allen - Cahn equation \n",
            "step :     0 , loss : 1.5728e-01 , Y0 :  5.4381e-01 , runtime :   13 \n",
            "step :   100 , loss : 1.2293e-01 ,  Y0 :  4.9526e-01 , runtime :   38 \n",
            "step :   200 , loss : 9.6781e-02 ,  Y0 :  4.5025e-01 , runtime :   55 \n",
            "step :   300 , loss : 7.6224e-02 ,  Y0 :  4.0867e-01 , runtime :   75 \n",
            "step :   400 , loss : 5.9557e-02 ,  Y0 :  3.7022e-01 , runtime :   94 \n",
            "step :   500 , loss : 4.6137e-02 ,  Y0 :  3.3448e-01 , runtime :  117 \n",
            "step :   600 , loss : 3.5656e-02 ,  Y0 :  3.0149e-01 , runtime :  134 \n",
            "step :   700 , loss : 2.7163e-02 ,  Y0 :  2.7099e-01 , runtime :  154 \n",
            "step :   800 , loss : 2.0474e-02 ,  Y0 :  2.4288e-01 , runtime :  178 \n",
            "step :   900 , loss : 1.5317e-02 ,  Y0 :  2.1719e-01 , runtime :  195 \n",
            "step :  1000 , loss : 1.1282e-02 ,  Y0 :  1.9379e-01 , runtime :  214 \n",
            "step :  1100 , loss : 8.1939e-03 ,  Y0 :  1.7269e-01 , runtime :  233 \n",
            "step :  1200 , loss : 5.8968e-03 ,  Y0 :  1.5376e-01 , runtime :  253 \n",
            "step :  1300 , loss : 4.1669e-03 ,  Y0 :  1.3706e-01 , runtime :  273 \n",
            "step :  1400 , loss : 2.9124e-03 ,  Y0 :  1.2233e-01 , runtime :  292 \n",
            "step :  1500 , loss : 2.0141e-03 ,  Y0 :  1.0942e-01 , runtime :  309 \n",
            "step :  1600 , loss : 1.3902e-03 ,  Y0 :  9.8354e-02 , runtime :  332 \n",
            "step :  1700 , loss : 9.7518e-04 ,  Y0 :  8.9110e-02 , runtime :  349 \n",
            "step :  1800 , loss : 7.0278e-04 ,  Y0 :  8.1286e-02 , runtime :  367 \n",
            "step :  1900 , loss : 5.1894e-04 ,  Y0 :  7.4730e-02 , runtime :  389 \n",
            "step :  2000 , loss : 4.0595e-04 ,  Y0 :  6.9413e-02 , runtime :  409 \n",
            "step :  2100 , loss : 3.3789e-04 ,  Y0 :  6.5235e-02 , runtime :  431 \n",
            "step :  2200 , loss : 2.9584e-04 ,  Y0 :  6.1907e-02 , runtime :  450 \n",
            "step :  2300 , loss : 2.7123e-04 ,  Y0 :  5.9350e-02 , runtime :  468 \n",
            "step :  2400 , loss : 2.5642e-04 ,  Y0 :  5.7465e-02 , runtime :  488 \n",
            "step :  2500 , loss : 2.4964e-04 ,  Y0 :  5.6044e-02 , runtime :  504 \n",
            "step :  2600 , loss : 2.4855e-04 ,  Y0 :  5.4980e-02 , runtime :  523 \n",
            "step :  2700 , loss : 2.4409e-04 ,  Y0 :  5.4240e-02 , runtime :  542 \n",
            "step :  2800 , loss : 2.4201e-04 ,  Y0 :  5.3927e-02 , runtime :  562 \n",
            "step :  2900 , loss : 2.3742e-04 ,  Y0 :  5.3612e-02 , runtime :  581 \n",
            "step :  3000 , loss : 2.3336e-04 ,  Y0 :  5.3447e-02 , runtime :  601 \n",
            "step :  3100 , loss : 2.3038e-04 ,  Y0 :  5.3174e-02 , runtime :  622 \n",
            "step :  3200 , loss : 2.2714e-04 ,  Y0 :  5.3118e-02 , runtime :  640 \n",
            "step :  3300 , loss : 2.2387e-04 ,  Y0 :  5.3129e-02 , runtime :  658 \n",
            "step :  3400 , loss : 2.2278e-04 ,  Y0 :  5.3009e-02 , runtime :  676 \n",
            "step :  3500 , loss : 2.2315e-04 ,  Y0 :  5.3021e-02 , runtime :  694 \n",
            "step :  3600 , loss : 2.2282e-04 ,  Y0 :  5.3137e-02 , runtime :  712 \n",
            "step :  3700 , loss : 2.1942e-04 ,  Y0 :  5.2959e-02 , runtime :  732 \n",
            "step :  3800 , loss : 2.1798e-04 ,  Y0 :  5.2821e-02 , runtime :  749 \n",
            "step :  3900 , loss : 2.1417e-04 ,  Y0 :  5.2788e-02 , runtime :  770 \n",
            "step :  4000 , loss : 2.0866e-04 ,  Y0 :  5.2983e-02 , runtime :  789 \n",
            " running time :  789.710 s \n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import math\n",
        "import tensorflow.compat.v1 as tf\n",
        "import numpy as np\n",
        "from tensorflow.python.training.moving_averages import assign_moving_average\n",
        "from scipy.stats import multivariate_normal as normal            # Generate normally distributed random numbers\n",
        "from tensorflow import random_normal_initializer as norm_init    #Initializers for generating tensors with normal distributions\n",
        "from tensorflow import random_uniform_initializer as unif_init   #Generating initializers with uniformly distributed tensors\n",
        "from tensorflow import constant_initializer as const_init        #Generating initializers for tensors with constant values\n",
        "import tensorflow.compat.v1 as tf\n",
        "tf.disable_v2_behavior()\n",
        "\n",
        "class SolveAllenCahn (object):\n",
        "    \"\"\" The fully-connected neural network model .\"\"\"\n",
        "    def __init__ (self,sess):\n",
        "        self.sess = sess\n",
        "        # parameters for the PDE\n",
        "        self.d = 100      # Dimensions of data\n",
        "        self.T = 0.3      # Length of time for each path\n",
        "        # parameters for the algorithm\n",
        "        self.n_time = 20     # Composed of 20 networks\n",
        "        self.n_layer = 4     # Number of layers of the neural network\n",
        "        self.n_neuron = [self.d ,self.d +10 ,self.d +10, self.d ]    # Number of neurons in each layer, corresponding to input, hidden layer 1, hidden layer 2, output\n",
        "        self.batch_size = 64      # Used one at a time for path calculation, 64*4=256\n",
        "        self.valid_size = 256     # 256 Monte Carlo samples (paths)\n",
        "        self.n_maxstep = 4000     #Iteration steps\n",
        "        self.n_displaystep = 100     # Print every 100 steps\n",
        "        self.learning_rate = 5e-4\n",
        "        self.Yini = [0.3,0.6]       # Maximum and minimum of initial value Y0\n",
        "        # some basic constants and variables\n",
        "        self.h = (self.T +0.0)/self.n_time    # Interval of data in each path，delta t\n",
        "        self.sqrth = math.sqrt(self.h)\n",
        "        self.t_stamp = np.arange(0,self.n_time)*self.h  # Timestamp, accumulated time\n",
        "        self._extra_train_ops = []  # batch moving average operation, which requires additional training of beta and gamma\n",
        "\n",
        "    def train(self):\n",
        "        # For training of neural networks\n",
        "        start_time = time.time()\n",
        "        # train operations create new tensorflow variable,name='global_step' with yield generator\n",
        "        self.global_step = \\\n",
        "            tf.get_variable('global_step', [] ,\n",
        "                              initializer = tf.constant_initializer(1),\n",
        "                              trainable = False,dtype = tf.int32 )\n",
        "        trainable_vars = tf.trainable_variables()\n",
        "        grads = tf.gradients(self.loss,trainable_vars)\n",
        "        optimizer = tf.train.AdamOptimizer(self.learning_rate)\n",
        "        apply_op = \\\n",
        "            optimizer.apply_gradients(zip(grads,trainable_vars) ,\n",
        "                                          global_step = self.global_step)\n",
        "\n",
        "        train_ops = [apply_op] + self._extra_train_ops\n",
        "        self.train_op = tf.group(* train_ops)\n",
        "\n",
        "        self.loss_history = []   # Used to record loss values\n",
        "        self.init_history = []   # Used to record the value of Y0\n",
        "        self.run_time = []\n",
        "\n",
        "        # for validation\n",
        "        dW_valid , X_valid = self.sample_path(self.valid_size)\n",
        "        feed_dict_valid = { self.dW : dW_valid,\n",
        "                            self.X : X_valid,\n",
        "                            self.is_training: False }\n",
        "        # initialization\n",
        "        step = 1\n",
        "        self.sess.run (tf.global_variables_initializer())\n",
        "\n",
        "        # Operational framework\n",
        "        temp_loss = self.sess.run(self.loss ,\n",
        "                                  feed_dict = feed_dict_valid )\n",
        "\n",
        "        temp_init = self.Y0.eval()[0]\n",
        "        self.loss_history.append(temp_loss)\n",
        "        self.init_history.append(temp_init)\n",
        "        self.run_time.append(time.time()-start_time + self.t_bd)\n",
        "        print(\"step : %5u , loss : %.4e , \" % \\\n",
        "                (0 ,temp_loss ) + \\\n",
        "                \"Y0 : % .4e , runtime : %4u \" % \\\n",
        "                (temp_init, time.time()-start_time + self.t_bd))\n",
        "\n",
        "        # begin sgd iteration，0-4000STEP\n",
        "        for _ in range (self.n_maxstep +1):\n",
        "            step = self.sess.run (self.global_step)\n",
        "            dW_train,X_train = self.sample_path(self.batch_size)\n",
        "            self.sess.run(self.train_op,\n",
        "                          feed_dict ={self.dW : dW_train ,\n",
        "                                      self.X : X_train ,\n",
        "                                      self.is_training : True })\n",
        "            if step % self.n_displaystep == 0:   # Test the loss and the value of Y0 every 100 steps with the validation set\n",
        "                temp_loss = self.sess.run(self.loss ,\n",
        "                                          feed_dict = feed_dict_valid)\n",
        "                temp_init = self.Y0.eval()[0]\n",
        "                self.loss_history.append(temp_loss)\n",
        "                self.init_history.append(temp_init )\n",
        "                self.run_time.append(time.time()-start_time + self.t_bd)\n",
        "                print(\"step : % 5u , loss : %.4e , \" % \\\n",
        "                        ( step , temp_loss ) + \\\n",
        "                        \" Y0 : % .4e , runtime : %4u \" % \\\n",
        "                        (temp_init , time.time() - start_time + self.t_bd ))\n",
        "            step += 1\n",
        "        end_time = time.time()\n",
        "        print(\" running time : % .3f s \" % \\\n",
        "                ( end_time - start_time + self.t_bd ))\n",
        "\n",
        "    def build(self):\n",
        "        # build the whole network by stacking subnetworks\n",
        "        start_time = time.time ()\n",
        "        # dW、X、is_training placeholder\n",
        "        self.dW = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time ] ,name = 'dW')   # None*100*20\n",
        "        self.X = tf.placeholder(tf.float32 ,[ None , self.d , self.n_time +1] ,name = 'X')   # None*100*20\n",
        "        self.is_training = tf.placeholder (tf.bool)\n",
        "\n",
        "        # Initialize Y0\\Z0\n",
        "        self.Y0 = tf.Variable(tf.random_uniform([1] ,                      # u0\n",
        "                                                minval = self.Yini [0] ,   # MIN0.3\n",
        "                                                maxval = self.Yini [1] ,   # MAX0.6\n",
        "                                                dtype = tf.float32 ));\n",
        "        self.Z0 = tf.Variable (tf.random_uniform ([1,self.d] ,    # The initial value of the u-gradient, a 1*d vector\n",
        "                                                minval = -.1 ,\n",
        "                                                maxval =.1 ,\n",
        "                                                dtype = tf.float32 ))\n",
        "        self.allones = \\\n",
        "             tf.ones(shape = tf.stack([ tf.shape(self.dW)[0],1]) ,   # tf.shape(self.dW)[0]=len(batch),shape=(batch,1)\n",
        "                         dtype = tf.float32 )\n",
        "\n",
        "        Y = self.allones * self.Y0\n",
        "        Z = tf.matmul(self.allones, self.Z0 )\n",
        "\n",
        "\n",
        "        with tf.variable_scope('forward'):\n",
        "            for t in range(0,self.n_time -1):\n",
        "\n",
        "                    Y = Y - self.f_tf(self.t_stamp[t] ,\n",
        "                                    self.X[:,:,t],Y,Z)* self.h\n",
        "                    Y = Y + tf.reduce_sum(Z * self.dW[:,:,t],1,\n",
        "                                       keep_dims = True )\n",
        "                    Z = self._one_time_net(self.X[:,:,t +1] ,\n",
        "                                       str(t +1))/self.d\n",
        "            # terminal time\n",
        "            Y = Y - self.f_tf(self.t_stamp[self.n_time -1] ,\n",
        "                                  self.X[:,:,self.n_time -1] ,\n",
        "                                  Y,Z)* self.h\n",
        "            Y = Y + tf.reduce_sum(Z * self.dW [:,:,self.n_time -1] , 1 ,\n",
        "                                      keep_dims = True )\n",
        "            term_delta = Y - self.g_tf(self.T,\n",
        "                                   self.X[:,:,self.n_time])\n",
        "            self.clipped_delta = \\\n",
        "                  tf.clip_by_value(term_delta ,-50.0 , 50.0)\n",
        "            self.loss = tf.reduce_mean(self.clipped_delta**2)\n",
        "        self.t_bd = time.time() - start_time\n",
        "\n",
        "    def sample_path(self, n_sample):\n",
        "        # （xt,(wt-wt-1)）\n",
        "        dW_sample = np.zeros([n_sample,self.d,self.n_time])\n",
        "        X_sample = np.zeros([n_sample,self.d,self.n_time +1])\n",
        "        for i in range(self.n_time):\n",
        "            dW_sample [:,:,i] = \\\n",
        "               np.reshape(normal.rvs(mean = np.zeros(self.d) ,\n",
        "                                     cov =1 ,\n",
        "                                     size = n_sample)* self.sqrth ,\n",
        "                          (n_sample,self.d))\n",
        "            X_sample[:,:,i +1] = X_sample[:,:,i] + \\\n",
        "                                   np.sqrt(2)*dW_sample[:,:,i]\n",
        "        return dW_sample, X_sample\n",
        "\n",
        "    def f_tf(self,t,X,Y,Z ):\n",
        "        # nonlinear term\n",
        "        return Y - tf.pow(Y,3)\n",
        "\n",
        "    def g_tf(self,t,X):\n",
        "        # terminal conditions\n",
        "        return 0.5/(1 + 0.2* tf.reduce_sum(X **2,1,keep_dims = True ))\n",
        "\n",
        "    def _one_time_net(self , x ,name ):\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            x_norm = self._batch_norm(x , name = 'layer0_normal')  # Normalize batch as input\n",
        "            layer1 = self._one_layer(x_norm , self.n_neuron [1] ,   # Hidden Layer 1 input(batch,d),output(batch，d+10)\n",
        "                                      name = 'layer1')\n",
        "            layer2 = self._one_layer(layer1,self.n_neuron[2] ,  # Hidden Layer 2 input(batch,d+10),output(batch,d+10)\n",
        "                                      name = 'layer2')\n",
        "            z = self._one_layer(layer2 , self.n_neuron [3] , #  input(batch,d+10),output(baatch,d)\n",
        "                                     activation_fn = None , name = 'final')\n",
        "        return z\n",
        "\n",
        "    def _one_layer(self , input_ , out_sz ,\n",
        "                   activation_fn = tf.nn.relu ,\n",
        "                   std =5.0 , name = 'linear'):\n",
        "\n",
        "        with tf.variable_scope(name):\n",
        "            shape = input_.get_shape().as_list()\n",
        "            w = tf.get_variable('Matrix',\n",
        "                                [shape[1], out_sz] ,tf.float32,\n",
        "                                norm_init(stddev = \\\n",
        "                                          std / np.sqrt(shape[1]+ out_sz )))\n",
        "            hidden = tf.matmul(input_ ,w )\n",
        "            hidden_bn = self._batch_norm(hidden, name = 'normal')\n",
        "        if activation_fn != None :\n",
        "            return activation_fn(hidden_bn)\n",
        "        else :\n",
        "            return hidden_bn\n",
        "\n",
        "    def _batch_norm(self , x , name ):\n",
        "        \"\"\" Batch normalization \"\"\"\n",
        "        with tf.variable_scope(name):\n",
        "            params_shape = [x.get_shape()[ -1]]   # [d,d+10,d+10,d]\n",
        "            beta = tf.get_variable('beta', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         norm_init(0.0 , stddev =0.1 ,\n",
        "                                         ))\n",
        "            gamma = tf.get_variable( 'gamma', params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         unif_init (0.1,0.5 ,\n",
        "                                          ))\n",
        "            mv_mean = tf.get_variable('moving_mean' ,\n",
        "                                         params_shape ,\n",
        "                                         tf.float32 ,\n",
        "                                         const_init (0.0) ,\n",
        "                                         trainable = False )\n",
        "            mv_var = tf.get_variable('moving_variance' ,\n",
        "                                        params_shape ,\n",
        "                                        tf.float32 ,\n",
        "                                        const_init(1.0) ,\n",
        "                                        trainable = False )\n",
        "\n",
        "            # These ops will only be preformed when training\n",
        "            mean ,variance = tf.nn.moments(x ,[0] , name = 'moments')\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_mean , mean , 0.99))\n",
        "            self._extra_train_ops.append (\\\n",
        "                 assign_moving_average(mv_var , variance , 0.99))\n",
        "\n",
        "            mean,variance = \\\n",
        "                tf.cond(self.is_training ,            # control_flow_ops.cond Controls the execution flow, with the first being a condition\n",
        "                                     lambda :( mean , variance ) ,\n",
        "                                     lambda :( mv_mean , mv_var ))\n",
        "\n",
        "            y = tf.nn.batch_normalization (x , mean , variance ,\n",
        "                                           beta , gamma , 1e-6)\n",
        "\n",
        "            y.set_shape( x.get_shape())\n",
        "            return y\n",
        "\n",
        "def main (name):\n",
        "    tf.reset_default_graph ()\n",
        "    with tf.Session() as sess :\n",
        "        tf.set_random_seed (1)\n",
        "        print(\" Begin to solve Allen - Cahn equation \")\n",
        "        model = SolveAllenCahn (sess)\n",
        "        model.build()\n",
        "        model.train ()\n",
        "        output = np.zeros ((len(model.init_history), 4))\n",
        "        output[:,0] = np.arange(len( model.init_history )) \\\n",
        "                             * model.n_displaystep\n",
        "        output[:,1] = model.loss_history\n",
        "        output[:,2] = model.init_history\n",
        "        output[:,3] = model.run_time\n",
        "        np.savetxt(\"./AllenCahn_d100\"+str(name)+\".csv\" ,  # Saving the output\n",
        "                     output ,\n",
        "                     fmt =[ '%d', '%.5e', '%.5e','%d'] ,\n",
        "                     delimiter =\",\",\n",
        "                     header =\"step ,loss function , \" + \\\n",
        "                     \" target value , runtime \" ,\n",
        "                     comments = '')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "        np.random.seed(1) # Define a random number seed\n",
        "        for i in range(5):\n",
        "            print(str(i)+' run:')\n",
        "            main(i)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openpyxl\n",
        "import pandas as pd\n",
        "import openpyxl\n",
        "from openpyxl.styles import PatternFill\n",
        "\n",
        "# Load the data\n",
        "dataframes = {}\n",
        "for i in range(5):\n",
        "    df = pd.read_csv(f'AllenCahn_d100{i}.csv')\n",
        "    df.columns = ['steps', f'loss_function{i+1}', f'target_value{i+1}', f'runtime{i+1}']\n",
        "    if i == 0:\n",
        "        merged_df = df\n",
        "    else:\n",
        "        merged_df = pd.merge(merged_df, df, on='steps', how='outer')\n",
        "\n",
        "# Calculate mean and standard deviation, ensuring precision\n",
        "merged_df['Mean_loss'] = merged_df[[f'loss_function{i+1}' for i in range(5)]].mean(axis=1).round(8)\n",
        "merged_df['Std_loss'] = merged_df[[f'loss_function{i+1}' for i in range(5)]].std(axis=1, ddof=0)  # ddof=0 for population standard deviation\n",
        "merged_df['Mean_Y0'] = merged_df[[f'target_value{i+1}' for i in range(5)]].mean(axis=1).round(7)\n",
        "merged_df['Std_Y0'] = merged_df[[f'target_value{i+1}' for i in range(5)]].std(axis=1, ddof=0)\n",
        "merged_df['Mean_time'] = merged_df[[f'runtime{i+1}' for i in range(5)]].mean(axis=1).round(1)\n",
        "merged_df['Std_time'] = merged_df[[f'runtime{i+1}' for i in range(5)]].std(axis=1, ddof=0)\n",
        "\n",
        "# Adjust the column order\n",
        "columns_order = [\n",
        "    'steps',\n",
        "    'loss_function1', 'loss_function2', 'loss_function3', 'loss_function4', 'loss_function5',\n",
        "    'Mean_loss', 'Std_loss',\n",
        "    'target_value1', 'target_value2', 'target_value3', 'target_value4', 'target_value5',\n",
        "    'Mean_Y0', 'Std_Y0',\n",
        "    'runtime1', 'runtime2', 'runtime3', 'runtime4', 'runtime5',\n",
        "    'Mean_time', 'Std_time'\n",
        "]\n",
        "merged_df = merged_df[columns_order]\n",
        "\n",
        "# Save the DataFrame to an Excel file\n",
        "merged_df.to_excel('wdl_data.xlsx', index=False)\n",
        "\n",
        "# Load the Excel file just saved\n",
        "wb = openpyxl.load_workbook('wdl_data.xlsx')\n",
        "ws = wb.active\n",
        "\n",
        "# Set the highlight format\n",
        "yellow_fill = PatternFill(start_color='FFFF00', end_color='FFFF00', fill_type='solid')\n",
        "\n",
        "# Highlight specific rows\n",
        "highlight_steps = [0, 1000, 2000, 3000, 4000]\n",
        "for row in ws.iter_rows():\n",
        "    if row[0].value in highlight_steps:\n",
        "        for cell in row:\n",
        "            # Set the background color to yellow\n",
        "            cell.fill = yellow_fill\n",
        "\n",
        "# Save the formatted Excel file\n",
        "wb.save('wdl_data.xlsx')\n",
        "\n",
        "# Load the Excel file\n",
        "data = pd.read_excel('wdl_data.xlsx')\n",
        "\n",
        "# Display the data\n",
        "print(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTGsOahQidCA",
        "outputId": "056df01a-6a06-4493-f10f-b74b7fad6f28"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.10/dist-packages (3.1.5)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.10/dist-packages (from openpyxl) (2.0.0)\n",
            "    steps  loss_function1  loss_function2  loss_function3  loss_function4  \\\n",
            "0       0        0.161578        0.159297        0.162758        0.165030   \n",
            "1     100        0.126918        0.124515        0.127561        0.128967   \n",
            "2     200        0.099422        0.097679        0.100189        0.101146   \n",
            "3     300        0.078032        0.076796        0.078726        0.079315   \n",
            "4     400        0.060838        0.060121        0.061394        0.062282   \n",
            "5     500        0.047267        0.046720        0.047542        0.048511   \n",
            "6     600        0.036247        0.035900        0.036464        0.037477   \n",
            "7     700        0.027612        0.027508        0.027747        0.028558   \n",
            "8     800        0.020776        0.020774        0.020873        0.021567   \n",
            "9     900        0.015425        0.015535        0.015542        0.016143   \n",
            "10   1000        0.011287        0.011440        0.011410        0.011925   \n",
            "11   1100        0.008187        0.008297        0.008231        0.008638   \n",
            "12   1200        0.005851        0.005926        0.005872        0.006195   \n",
            "13   1300        0.004132        0.004197        0.004126        0.004393   \n",
            "14   1400        0.002876        0.002935        0.002873        0.003088   \n",
            "15   1500        0.001999        0.002030        0.001969        0.002165   \n",
            "16   1600        0.001364        0.001409        0.001365        0.001502   \n",
            "17   1700        0.000946        0.000987        0.000960        0.001058   \n",
            "18   1800        0.000671        0.000712        0.000693        0.000750   \n",
            "19   1900        0.000493        0.000531        0.000525        0.000555   \n",
            "20   2000        0.000384        0.000418        0.000426        0.000432   \n",
            "21   2100        0.000322        0.000354        0.000369        0.000358   \n",
            "22   2200        0.000285        0.000311        0.000335        0.000309   \n",
            "23   2300        0.000265        0.000291        0.000315        0.000283   \n",
            "24   2400        0.000253        0.000280        0.000308        0.000267   \n",
            "25   2500        0.000247        0.000275        0.000305        0.000257   \n",
            "26   2600        0.000244        0.000269        0.000304        0.000250   \n",
            "27   2700        0.000236        0.000266        0.000300        0.000249   \n",
            "28   2800        0.000234        0.000264        0.000298        0.000244   \n",
            "29   2900        0.000236        0.000262        0.000293        0.000241   \n",
            "30   3000        0.000235        0.000262        0.000289        0.000239   \n",
            "31   3100        0.000231        0.000256        0.000287        0.000236   \n",
            "32   3200        0.000229        0.000255        0.000283        0.000233   \n",
            "33   3300        0.000225        0.000253        0.000281        0.000231   \n",
            "34   3400        0.000222        0.000249        0.000273        0.000227   \n",
            "35   3500        0.000220        0.000245        0.000270        0.000229   \n",
            "36   3600        0.000219        0.000239        0.000269        0.000223   \n",
            "37   3700        0.000213        0.000238        0.000268        0.000218   \n",
            "38   3800        0.000212        0.000235        0.000265        0.000217   \n",
            "39   3900        0.000207        0.000231        0.000259        0.000212   \n",
            "40   4000        0.000198        0.000225        0.000249        0.000210   \n",
            "\n",
            "    loss_function5  Mean_loss  Std_loss  target_value1  target_value2  ...  \\\n",
            "0         0.157275   0.161188  0.002693       0.543809       0.543809  ...   \n",
            "1         0.122933   0.126179  0.002170       0.495161       0.495328  ...   \n",
            "2         0.096782   0.099044  0.001603       0.450036       0.450503  ...   \n",
            "3         0.076224   0.077818  0.001157       0.408321       0.408909  ...   \n",
            "4         0.059557   0.060838  0.000954       0.369753       0.370218  ...   \n",
            "5         0.046137   0.047235  0.000799       0.334010       0.334422  ...   \n",
            "6         0.035655   0.036349  0.000629       0.301031       0.301374  ...   \n",
            "7         0.027163   0.027718  0.000463       0.270482       0.270904  ...   \n",
            "8         0.020474   0.020893  0.000363       0.242413       0.242767  ...   \n",
            "9         0.015317   0.015592  0.000287       0.216701       0.217184  ...   \n",
            "10        0.011282   0.011469  0.000237       0.193328       0.193806  ...   \n",
            "11        0.008194   0.008310  0.000169       0.172307       0.172614  ...   \n",
            "12        0.005897   0.005948  0.000126       0.153465       0.153786  ...   \n",
            "13        0.004167   0.004203  0.000098       0.136737       0.136946  ...   \n",
            "14        0.002912   0.002937  0.000079       0.122065       0.122195  ...   \n",
            "15        0.002014   0.002035  0.000068       0.109313       0.109360  ...   \n",
            "16        0.001390   0.001406  0.000051       0.098167       0.098318  ...   \n",
            "17        0.000975   0.000985  0.000039       0.088811       0.088880  ...   \n",
            "18        0.000703   0.000706  0.000026       0.081034       0.081026  ...   \n",
            "19        0.000519   0.000525  0.000020       0.074495       0.074552  ...   \n",
            "20        0.000406   0.000413  0.000017       0.069254       0.069285  ...   \n",
            "21        0.000338   0.000348  0.000016       0.065190       0.065125  ...   \n",
            "22        0.000296   0.000307  0.000017       0.061910       0.061826  ...   \n",
            "23        0.000271   0.000285  0.000018       0.059353       0.059362  ...   \n",
            "24        0.000256   0.000273  0.000020       0.057424       0.057297  ...   \n",
            "25        0.000250   0.000267  0.000022       0.056086       0.055945  ...   \n",
            "26        0.000249   0.000263  0.000022       0.055164       0.055077  ...   \n",
            "27        0.000244   0.000259  0.000023       0.054373       0.054382  ...   \n",
            "28        0.000242   0.000257  0.000023       0.053833       0.053791  ...   \n",
            "29        0.000237   0.000254  0.000022       0.053552       0.053385  ...   \n",
            "30        0.000233   0.000252  0.000021       0.053180       0.053317  ...   \n",
            "31        0.000230   0.000248  0.000022       0.053086       0.053162  ...   \n",
            "32        0.000227   0.000245  0.000021       0.053076       0.053171  ...   \n",
            "33        0.000224   0.000243  0.000022       0.052934       0.052863  ...   \n",
            "34        0.000223   0.000239  0.000020       0.053084       0.052963  ...   \n",
            "35        0.000223   0.000237  0.000019       0.053002       0.052924  ...   \n",
            "36        0.000223   0.000235  0.000019       0.053008       0.052895  ...   \n",
            "37        0.000219   0.000231  0.000020       0.053000       0.052717  ...   \n",
            "38        0.000218   0.000229  0.000019       0.052902       0.052642  ...   \n",
            "39        0.000214   0.000224  0.000019       0.052922       0.052787  ...   \n",
            "40        0.000209   0.000218  0.000018       0.053184       0.053060  ...   \n",
            "\n",
            "    target_value5   Mean_Y0    Std_Y0  runtime1  runtime2  runtime3  runtime4  \\\n",
            "0        0.543809  0.543809  0.000000        16        13        13        14   \n",
            "1        0.495264  0.495238  0.000059        44        40        40        38   \n",
            "2        0.450254  0.450293  0.000156        63        61        60        58   \n",
            "3        0.408671  0.408691  0.000204        84        82        83        79   \n",
            "4        0.370223  0.370103  0.000180       107       103       104        98   \n",
            "5        0.334478  0.334330  0.000165       130       122       120       117   \n",
            "6        0.301493  0.301273  0.000162       150       143       141       137   \n",
            "7        0.270987  0.270753  0.000178       168       164       160       158   \n",
            "8        0.242880  0.242675  0.000156       190       185       182       179   \n",
            "9        0.217194  0.216989  0.000190       209       206       198       200   \n",
            "10       0.193787  0.193607  0.000182       226       226       217       215   \n",
            "11       0.172689  0.172484  0.000153       246       245       237       237   \n",
            "12       0.153761  0.153605  0.000159       266       264       255       254   \n",
            "13       0.137060  0.136828  0.000166       287       286       274       275   \n",
            "14       0.122329  0.122106  0.000146       302       303       298       295   \n",
            "15       0.109417  0.109301  0.000106       322       326       315       314   \n",
            "16       0.098354  0.098241  0.000102       338       342       336       334   \n",
            "17       0.089110  0.088916  0.000109       358       360       356       351   \n",
            "18       0.081286  0.081070  0.000118       376       378       376       371   \n",
            "19       0.074731  0.074570  0.000085       397       401       395       391   \n",
            "20       0.069413  0.069323  0.000070       418       423       413       410   \n",
            "21       0.065235  0.065206  0.000063       437       443       435       426   \n",
            "22       0.061907  0.061902  0.000046       454       463       456       446   \n",
            "23       0.059350  0.059391  0.000082       471       486       476       465   \n",
            "24       0.057465  0.057452  0.000090       489       505       495       483   \n",
            "25       0.056044  0.056070  0.000072       506       527       513       503   \n",
            "26       0.054981  0.055074  0.000059       525       547       532       523   \n",
            "27       0.054240  0.054390  0.000088       545       564       549       543   \n",
            "28       0.053928  0.053871  0.000050       565       582       568       563   \n",
            "29       0.053612  0.053522  0.000086       585       602       586       583   \n",
            "30       0.053447  0.053309  0.000089       600       621       603       605   \n",
            "31       0.053174  0.053139  0.000033       619       639       623       624   \n",
            "32       0.053118  0.053155  0.000054       636       658       641       645   \n",
            "33       0.053129  0.052981  0.000089       656       680       660       665   \n",
            "34       0.053010  0.052995  0.000058       677       697       681       684   \n",
            "35       0.053021  0.052905  0.000101       696       715       702       705   \n",
            "36       0.053137  0.052897  0.000195       714       734       723       722   \n",
            "37       0.052959  0.052814  0.000138       735       754       746       742   \n",
            "38       0.052821  0.052831  0.000099       759       772       765       761   \n",
            "39       0.052788  0.052855  0.000107       776       790       787       777   \n",
            "40       0.052983  0.052992  0.000158       793       811       803       798   \n",
            "\n",
            "    runtime5  Mean_time  Std_time  \n",
            "0         13       13.8  1.166190  \n",
            "1         38       40.0  2.190890  \n",
            "2         55       59.4  2.727636  \n",
            "3         75       80.6  3.261901  \n",
            "4         94      101.2  4.621688  \n",
            "5        117      121.2  4.791659  \n",
            "6        134      141.0  5.477226  \n",
            "7        154      160.8  4.833218  \n",
            "8        178      182.8  4.354308  \n",
            "9        195      201.6  5.161395  \n",
            "10       214      219.6  5.314132  \n",
            "11       233      239.6  5.043808  \n",
            "12       253      258.4  5.462600  \n",
            "13       273      279.0  6.164414  \n",
            "14       292      298.0  4.147288  \n",
            "15       309      317.2  6.046487  \n",
            "16       332      336.4  3.440930  \n",
            "17       349      354.8  4.166533  \n",
            "18       367      373.6  4.029888  \n",
            "19       389      394.6  4.270831  \n",
            "20       409      414.6  5.238320  \n",
            "21       431      434.4  5.713143  \n",
            "22       450      453.8  5.741080  \n",
            "23       468      473.2  7.359348  \n",
            "24       488      492.0  7.536577  \n",
            "25       504      510.6  8.912912  \n",
            "26       523      530.0  9.121403  \n",
            "27       542      548.6  8.064738  \n",
            "28       562      568.0  7.293833  \n",
            "29       581      587.4  7.499333  \n",
            "30       601      606.0  7.694154  \n",
            "31       622      625.4  7.002857  \n",
            "32       640      644.0  7.563068  \n",
            "33       658      663.8  8.634813  \n",
            "34       676      683.0  7.563068  \n",
            "35       694      702.4  7.445804  \n",
            "36       712      721.0  7.797435  \n",
            "37       732      741.8  7.858753  \n",
            "38       749      761.2  7.547185  \n",
            "39       770      780.0  7.402702  \n",
            "40       789      798.8  7.704544  \n",
            "\n",
            "[41 rows x 22 columns]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}